--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950180-Revert-ARM-dma-mapping-remove-dmac_clean_range-and-d.patch	2024-12-16 20:30:08.152889251 +0800
@@ -0,0 +1,159 @@
+From 9286d857a8a538f4bc6c14996327a8b653ad88a7 Mon Sep 17 00:00:00 2001
+From: Kathiravan T <kathirav@codeaurora.org>
+Date: Thu, 7 Feb 2013 12:15:11 -0800
+Subject: [PATCH 094/500] Revert "ARM: dma-mapping: remove dmac_clean_range and
+ dmac_inv_range"
+
+This partially reverts 'commit 702b94bff3c505 ("ARM: dma-mapping:
+remove dmac_clean_range and dmac_inv_range")'
+
+Some MSM drivers still use the dmac_clean and dmac_inv_range APIs.
+Bring back the defines and exports for v7 CPUs.
+
+Change-Id: I69017d73da1065a5eeb9c87c899b6a51be5ebfe6
+Signed-off-by: Rohit Vaswani <rvaswani@codeaurora.org>
+Signed-off-by: Abhimanyu Kapur <abhimany@codeaurora.org>
+[sramana: resolved minor merge conflicts]
+Signed-off-by: Srinivas Ramana <sramana@codeaurora.org>
+[akdwived: Made change for checkpatch error]
+Signed-off-by: Avaneesh Kumar Dwivedi <akdwived@codeaurora.org>
+Signed-off-by: Vijayanand Jitta <vjitta@codeaurora.org>
+[qqzhou@codeaurora.org: Made change for compile error]
+Signed-off-by: Qingqing Zhou <qqzhou@codeaurora.org>
+
+(cherry picked from commit 432d2898642443b498747a8e8a42cb423bd726f1)
+Signed-off-by: Kathiravan T <kathirav@codeaurora.org>
+
+Change-Id: If81758c05ae89333205b8021a73287920a8ee64d
+Signed-off-by: Pavithra R <quic_pavir@quicinc.com>
+---
+ arch/arm/include/asm/cacheflush.h | 21 +++++++++++++++++++++
+ arch/arm/include/asm/glue-cache.h |  2 ++
+ arch/arm/mm/cache-v7.S            |  6 ++++--
+ arch/arm/mm/proc-macros.S         |  4 +++-
+ arch/arm/mm/proc-syms.c           |  3 +++
+ 5 files changed, 33 insertions(+), 3 deletions(-)
+
+--- a/arch/arm/include/asm/cacheflush.h
++++ b/arch/arm/include/asm/cacheflush.h
+@@ -91,6 +91,21 @@
+  *	DMA Cache Coherency
+  *	===================
+  *
++ *	dma_inv_range(start, end)
++ *
++ *		Invalidate (discard) the specified virtual address range.
++ *		May not write back any entries.  If 'start' or 'end'
++ *		are not cache line aligned, those lines must be written
++ *		back.
++ *		- start  - virtual start address
++ *		- end    - virtual end address
++ *
++ *	dma_clean_range(start, end)
++ *
++ *		Clean (write back) the specified virtual address range.
++ *		- start  - virtual start address
++ *		- end    - virtual end address
++ *
+  *	dma_flush_range(start, end)
+  *
+  *		Clean and invalidate the specified virtual address range.
+@@ -112,6 +127,8 @@ struct cpu_cache_fns {
+ 	void (*dma_map_area)(const void *, size_t, int);
+ 	void (*dma_unmap_area)(const void *, size_t, int);
+ 
++	void (*dma_inv_range)(const void *, const void *);
++	void (*dma_clean_range)(const void *, const void *);
+ 	void (*dma_flush_range)(const void *, const void *);
+ } __no_randomize_layout;
+ 
+@@ -137,6 +154,8 @@ extern struct cpu_cache_fns cpu_cache;
+  * is visible to DMA, or data written by DMA to system memory is
+  * visible to the CPU.
+  */
++#define dmac_inv_range			cpu_cache.dma_inv_range
++#define dmac_clean_range		cpu_cache.dma_clean_range
+ #define dmac_flush_range		cpu_cache.dma_flush_range
+ 
+ #else
+@@ -156,6 +175,8 @@ extern void __cpuc_flush_dcache_area(voi
+  * is visible to DMA, or data written by DMA to system memory is
+  * visible to the CPU.
+  */
++extern void dmac_inv_range(const void *, const void *);
++extern void dmac_clean_range(const void *, const void *);
+ extern void dmac_flush_range(const void *, const void *);
+ 
+ #endif
+--- a/arch/arm/include/asm/glue-cache.h
++++ b/arch/arm/include/asm/glue-cache.h
+@@ -156,6 +156,8 @@ static inline void nop_dma_unmap_area(co
+ #define __cpuc_flush_dcache_area	__glue(_CACHE,_flush_kern_dcache_area)
+ 
+ #define dmac_flush_range		__glue(_CACHE,_dma_flush_range)
++#define dmac_inv_range			__glue(_CACHE, _dma_inv_range)
++#define dmac_clean_range		__glue(_CACHE, _dma_clean_range)
+ #endif
+ 
+ #endif
+--- a/arch/arm/mm/cache-v7.S
++++ b/arch/arm/mm/cache-v7.S
+@@ -361,7 +361,7 @@ ENDPROC(v7_flush_kern_dcache_area)
+  *	- start   - virtual start address of region
+  *	- end     - virtual end address of region
+  */
+-v7_dma_inv_range:
++ENTRY(v7_dma_inv_range)
+ 	dcache_line_size r2, r3
+ 	sub	r3, r2, #1
+ 	tst	r0, r3
+@@ -391,7 +391,7 @@ ENDPROC(v7_dma_inv_range)
+  *	- start   - virtual start address of region
+  *	- end     - virtual end address of region
+  */
+-v7_dma_clean_range:
++ENTRY(v7_dma_clean_range)
+ 	dcache_line_size r2, r3
+ 	sub	r3, r2, #1
+ 	bic	r0, r0, r3
+@@ -477,6 +477,8 @@ ENDPROC(v7_dma_unmap_area)
+ 
+ 	globl_equ	b15_dma_map_area,		v7_dma_map_area
+ 	globl_equ	b15_dma_unmap_area,		v7_dma_unmap_area
++	globl_equ	b15_dma_inv_range,		v7_dma_inv_range
++	globl_equ	b15_dma_clean_range,		v7_dma_clean_range
+ 	globl_equ	b15_dma_flush_range,		v7_dma_flush_range
+ 
+ 	define_cache_functions b15
+--- a/arch/arm/mm/proc-macros.S
++++ b/arch/arm/mm/proc-macros.S
+@@ -326,7 +326,7 @@ ENTRY(\name\()_processor_functions)
+ ENTRY(\name\()_cache_fns)
+ 	.long	\name\()_flush_icache_all
+ 	.long	\name\()_flush_kern_cache_all
+-	.long   \name\()_flush_kern_cache_louis
++	.long	\name\()_flush_kern_cache_louis
+ 	.long	\name\()_flush_user_cache_all
+ 	.long	\name\()_flush_user_cache_range
+ 	.long	\name\()_coherent_kern_range
+@@ -334,6 +334,8 @@ ENTRY(\name\()_cache_fns)
+ 	.long	\name\()_flush_kern_dcache_area
+ 	.long	\name\()_dma_map_area
+ 	.long	\name\()_dma_unmap_area
++	.long	\name\()_dma_inv_range
++	.long	\name\()_dma_clean_range
+ 	.long	\name\()_dma_flush_range
+ 	.size	\name\()_cache_fns, . - \name\()_cache_fns
+ .endm
+--- a/arch/arm/mm/proc-syms.c
++++ b/arch/arm/mm/proc-syms.c
+@@ -27,6 +27,9 @@ EXPORT_SYMBOL(__cpuc_flush_user_all);
+ EXPORT_SYMBOL(__cpuc_flush_user_range);
+ EXPORT_SYMBOL(__cpuc_coherent_kern_range);
+ EXPORT_SYMBOL(__cpuc_flush_dcache_area);
++EXPORT_SYMBOL(dmac_inv_range);
++EXPORT_SYMBOL(dmac_clean_range);
++EXPORT_SYMBOL(dmac_flush_range);
+ #else
+ EXPORT_SYMBOL(cpu_cache);
+ #endif
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950184-net-add-fast-xmit-api.patch	2024-12-16 20:31:54.401170653 +0800
@@ -0,0 +1,109 @@
+From 04d249769b277035aeaac85ac2a73a85ec163fcb Mon Sep 17 00:00:00 2001
+From: Ken Zhu <quic_guigenz@quicinc.com>
+Date: Tue, 25 Jan 2022 16:09:56 -0800
+Subject: [PATCH 178/500] net: add fast xmit api
+
+this new function bypass most validation on the skb, and call
+netdev_start_xmit directly from this function.
+
+Change-Id: Ifc9019370a7a3839945ca83e24d675fd898c23c1
+Signed-off-by: Ken Zhu <quic_guigenz@quicinc.com>
+---
+ include/linux/netdevice.h |  2 ++
+ net/core/dev.c            | 74 +++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 76 insertions(+)
+
+--- a/include/linux/netdevice.h
++++ b/include/linux/netdevice.h
+@@ -3155,6 +3155,8 @@ static inline int dev_direct_xmit(struct
+ 	return ret;
+ }
+ 
++bool dev_fast_xmit(struct sk_buff *skb, struct net_device *dev,
++		   netdev_features_t features);
+ int register_netdevice(struct net_device *dev);
+ void unregister_netdevice_queue(struct net_device *dev, struct list_head *head);
+ void unregister_netdevice_many(struct list_head *head);
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -4296,6 +4296,80 @@ struct netdev_queue *netdev_core_pick_tx
+ }
+ 
+ /**
++ *	dev_fast_xmit - fast xmit the skb
++ *	@skb:buffer to transmit
++ *	@dev: the device to be transmited to
++ *	@features: the skb features could bed used
++ *	sucessful return true
++ *	failed return false
++ */
++bool dev_fast_xmit(struct sk_buff *skb,
++		struct net_device *dev,
++		netdev_features_t features)
++{
++	struct netdev_queue *txq;
++	int cpu;
++	netdev_tx_t rc;
++
++	if (unlikely(!(dev->flags & IFF_UP))) {
++		return false;
++	}
++
++	if (unlikely(skb_needs_linearize(skb, features))) {
++		return false;
++	}
++
++	rcu_read_lock_bh();
++	cpu = smp_processor_id();
++
++	/* If device don't need the dst, release it now, otherwise make sure
++	 * the refcount increased.
++	 */
++	if (likely(dev->priv_flags & IFF_XMIT_DST_RELEASE)) {
++		skb_dst_drop(skb);
++	} else {
++		skb_dst_force(skb);
++	}
++
++	txq = netdev_core_pick_tx(dev, skb, NULL);
++
++	if (likely(txq->xmit_lock_owner != cpu)) {
++#define FAST_HARD_TX_LOCK(features, txq, cpu) {		\
++	if ((features & NETIF_F_LLTX) == 0) {		\
++		__netif_tx_lock(txq, cpu);		\
++	} else {					\
++		__netif_tx_acquire(txq);		\
++	}						\
++}
++
++#define FAST_HARD_TX_UNLOCK(features, txq) {		\
++	if ((features & NETIF_F_LLTX) == 0) {		\
++		__netif_tx_unlock(txq);			\
++	} else {					\
++		__netif_tx_release(txq);		\
++	}						\
++}
++		netdev_features_t dev_features = dev->features;
++		FAST_HARD_TX_LOCK(dev_features, txq, cpu);
++		if (likely(!netif_xmit_stopped(txq))) {
++			rc = netdev_start_xmit(skb, dev, txq, 0);
++			if (unlikely(!dev_xmit_complete(rc))) {
++				FAST_HARD_TX_UNLOCK(dev_features, txq);
++				goto fail;
++			}
++			FAST_HARD_TX_UNLOCK(dev_features, txq);
++			rcu_read_unlock_bh();
++			return true;
++		}
++		FAST_HARD_TX_UNLOCK(dev_features, txq);
++	}
++fail:
++	rcu_read_unlock_bh();
++	return false;
++}
++EXPORT_SYMBOL(dev_fast_xmit);
++
++/**
+  * __dev_queue_xmit() - transmit a buffer
+  * @skb:	buffer to transmit
+  * @sb_dev:	suboordinate device used for L2 forwarding offload
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950185-net-set-skb-s-fast_xmit-flag-in-dev_fast_xmit-API.patch	2024-12-16 20:31:54.526168635 +0800
@@ -0,0 +1,39 @@
+From 61b8596727cb75c134ca6367b956d0e4596bfc69 Mon Sep 17 00:00:00 2001
+From: Tallapragada Kalyan <quic_ktallapr@quicinc.com>
+Date: Thu, 9 Jun 2022 09:32:38 +0530
+Subject: [PATCH 179/500] net: set skb's fast_xmit flag in dev_fast_xmit API
+
+set skb's fast_xmit flag in dev_fast_xmit API for linear packets
+WiFi tx path can avoid some overhead due to checks based on this flag
+
+Change-Id: Ied29f9d615d0cf48dd9dcd7fcf0fb210eb259a8f
+Signed-off-by: Tallapragada Kalyan <quic_ktallapr@quicinc.com>
+---
+ include/linux/skbuff.h | 2 ++
+ net/core/dev.c         | 4 ++++
+ 2 files changed, 6 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -984,6 +984,8 @@ struct sk_buff {
+ #if IS_ENABLED(CONFIG_IP_SCTP)
+ 	__u8			csum_not_inet:1;
+ #endif
++	/* Linear packets processed by dev_fast_xmit() */
++	__u8			fast_xmit:1;
+ 
+ #if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)
+ 	__u16			tc_index;	/* traffic control index */
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -4311,6 +4311,10 @@ bool dev_fast_xmit(struct sk_buff *skb,
+ 	int cpu;
+ 	netdev_tx_t rc;
+ 
++	/* the fast_xmit flag will avoid multiple checks in wifi xmit path */
++	if (likely(!skb_is_nonlinear(skb)))
++		skb->fast_xmit = 1;
++
+ 	if (unlikely(!(dev->flags & IFF_UP))) {
+ 		return false;
+ 	}
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950186-net_device-Initialized-shadow-address-for-address-ch.patch	2024-12-16 20:30:08.156889187 +0800
@@ -0,0 +1,35 @@
+From 7e9e3a73fc9747564918a754ea82974bed1e32c0 Mon Sep 17 00:00:00 2001
+From: Tian Yang <quic_tiany@quicinc.com>
+Date: Thu, 27 Jul 2023 00:42:04 -0700
+Subject: [PATCH 180/500] [net_device] Initialized shadow address for address
+ check
+
+Shadow address needs to be initialized once dev_addr is set.
+
+Change-Id: Ib60de68d148506db7d0fb1f26ee7efe3a1c3a729
+Signed-off-by: Tian Yang <quic_tiany@quicinc.com>
+---
+ net/core/dev.c            | 1 -
+ net/core/dev_addr_lists.c | 1 +
+ 2 files changed, 1 insertion(+), 1 deletion(-)
+
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -1471,7 +1471,6 @@ static int __dev_open(struct net_device
+ 	int ret;
+ 
+ 	ASSERT_RTNL();
+-	dev_addr_check(dev);
+ 
+ 	if (!netif_device_present(dev)) {
+ 		/* may be detached because parent is runtime-suspended */
+--- a/net/core/dev_addr_lists.c
++++ b/net/core/dev_addr_lists.c
+@@ -565,6 +565,7 @@ int dev_addr_init(struct net_device *dev
+ 		ha = list_first_entry(&dev->dev_addrs.list,
+ 				      struct netdev_hw_addr, list);
+ 		dev->dev_addr = ha->addr;
++		memcpy(dev->dev_addr_shadow, dev->dev_addr, MAX_ADDR_LEN);
+ 	}
+ 	return err;
+ }
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950187-arm-arm64-Add-new-APIs-to-perform-dma-cache-maintena.patch	2024-12-16 20:30:08.157889170 +0800
@@ -0,0 +1,379 @@
+From da352a66ee871ddba0e1970f029ca17d5b4fd99b Mon Sep 17 00:00:00 2001
+From: Suman Ghosh <quic_sumaghos@quicinc.com>
+Date: Sat, 4 Sep 2021 01:09:20 +0530
+Subject: [PATCH 095/500] arm/arm64: Add new APIs to perform dma cache
+ maintenance operation without dsb.
+
+Change-Id: I511657af343c8dc668ab7280362b3cdd57579360
+Signed-off-by: Suman Ghosh <quic_sumaghos@quicinc.com>
+Signed-off-by: Tushar Ganatra <quic_tganatra@quicinc.com>
+Signed-off-by: Pavithra R <quic_pavir@quicinc.com>
+Signed-off-by: Tushar Ganatra <quic_tganatra@quicinc.com>
+---
+ arch/arm/include/asm/cacheflush.h   | 11 ++++
+ arch/arm/include/asm/glue-cache.h   |  3 +
+ arch/arm/mm/cache-v7.S              | 98 +++++++++++++++++++++++++++--
+ arch/arm/mm/proc-syms.c             |  3 +
+ arch/arm64/include/asm/assembler.h  | 39 ++++++++++++
+ arch/arm64/include/asm/cacheflush.h |  9 +++
+ arch/arm64/mm/cache.S               | 58 +++++++++++++++++
+ arch/arm64/mm/flush.c               | 24 +++++++
+ 8 files changed, 239 insertions(+), 6 deletions(-)
+
+--- a/arch/arm/include/asm/cacheflush.h
++++ b/arch/arm/include/asm/cacheflush.h
+@@ -130,6 +130,10 @@ struct cpu_cache_fns {
+ 	void (*dma_inv_range)(const void *, const void *);
+ 	void (*dma_clean_range)(const void *, const void *);
+ 	void (*dma_flush_range)(const void *, const void *);
++
++	void (*dma_inv_range_no_dsb)(const void *, const void *);
++	void (*dma_clean_range_no_dsb)(const void *, const void *);
++	void (*dma_flush_range_no_dsb)(const void *, const void *);
+ } __no_randomize_layout;
+ 
+ /*
+@@ -158,6 +162,10 @@ extern struct cpu_cache_fns cpu_cache;
+ #define dmac_clean_range		cpu_cache.dma_clean_range
+ #define dmac_flush_range		cpu_cache.dma_flush_range
+ 
++#define dmac_inv_range_no_dsb	cpu_cache.dma_inv_range_no_dsb
++#define dmac_clean_range_no_dsb	cpu_cache.dma_clean_range_no_dsb
++#define dmac_flush_range_no_dsb	cpu_cache.dma_flush_range_no_dsb
++
+ #else
+ 
+ extern void __cpuc_flush_icache_all(void);
+@@ -178,6 +186,9 @@ extern void __cpuc_flush_dcache_area(voi
+ extern void dmac_inv_range(const void *, const void *);
+ extern void dmac_clean_range(const void *, const void *);
+ extern void dmac_flush_range(const void *, const void *);
++extern void dmac_inv_range_no_dsb(const void *, const void *);
++extern void dmac_clean_range_no_dsb(const void *, const void *);
++extern void dmac_flush_range_no_dsb(const void *, const void *);
+ 
+ #endif
+ 
+--- a/arch/arm/include/asm/glue-cache.h
++++ b/arch/arm/include/asm/glue-cache.h
+@@ -156,8 +156,11 @@ static inline void nop_dma_unmap_area(co
+ #define __cpuc_flush_dcache_area	__glue(_CACHE,_flush_kern_dcache_area)
+ 
+ #define dmac_flush_range		__glue(_CACHE,_dma_flush_range)
++#define dmac_flush_range_no_dsb	__glue(_CACHE,_dma_flush_range_no_dsb)
+ #define dmac_inv_range			__glue(_CACHE, _dma_inv_range)
++#define dmac_inv_range_no_dsb	__glue(_CACHE, _dma_inv_range_no_dsb)
+ #define dmac_clean_range		__glue(_CACHE, _dma_clean_range)
++#define dmac_clean_range_no_dsb	__glue(_CACHE, _dma_clean_range_no_dsb)
+ #endif
+ 
+ #endif
+--- a/arch/arm/mm/cache-v7.S
++++ b/arch/arm/mm/cache-v7.S
+@@ -387,6 +387,42 @@ ENTRY(v7_dma_inv_range)
+ ENDPROC(v7_dma_inv_range)
+ 
+ /*
++ * v7_dma_inv_range_no_dsb(start,end)
++ *
++ *	Invalidate the data cache within the specified region; we will
++ *	be performing a DMA operation in this region and we want to
++ *	purge old data in the cache. This API does not do
++ *	"data synchronization barrier". The caller is responsible to
++ *	do dsb after the transaction.
++ *
++ *	- start   - virtual start address of region
++ *	- end     - virtual end address of region
++ */
++ENTRY(v7_dma_inv_range_no_dsb)
++	dcache_line_size r2, r3
++	sub	r3, r2, #1
++	tst	r0, r3
++	bic	r0, r0, r3
++#ifdef CONFIG_ARM_ERRATA_764369
++	ALT_SMP(W(dsb))
++	ALT_UP(W(nop))
++#endif
++	mcrne	p15, 0, r0, c7, c14, 1		@ clean & invalidate D / U line
++	addne	r0, r0, r2
++
++	tst	r1, r3
++	bic	r1, r1, r3
++	mcrne	p15, 0, r1, c7, c14, 1		@ clean & invalidate D / U line
++	cmp	r0, r1
++1:
++	mcrlo	p15, 0, r0, c7, c6, 1		@ invalidate D / U line
++	addlo	r0, r0, r2
++	cmplo	r0, r1
++	blo	1b
++	ret	lr
++ENDPROC(v7_dma_inv_range_no_dsb)
++
++/*
+  *	v7_dma_clean_range(start,end)
+  *	- start   - virtual start address of region
+  *	- end     - virtual end address of region
+@@ -409,12 +445,16 @@ ENTRY(v7_dma_clean_range)
+ ENDPROC(v7_dma_clean_range)
+ 
+ /*
+- *	v7_dma_flush_range(start,end)
++ *  v7_dma_clean_range_no_dsb(start,end)
++ *
++ *	This API does not do "data synchronization barrier".
++ *	The caller is responsible to do dsb after the transaction.
++ *
+  *	- start   - virtual start address of region
+  *	- end     - virtual end address of region
+  */
+-ENTRY(v7_dma_flush_range)
+-	dcache_line_size r2, r3
++ENTRY(v7_dma_clean_range_no_dsb)
++	dcache_line_size	r2, r3
+ 	sub	r3, r2, #1
+ 	bic	r0, r0, r3
+ #ifdef CONFIG_ARM_ERRATA_764369
+@@ -422,6 +462,27 @@ ENTRY(v7_dma_flush_range)
+ 	ALT_UP(W(nop))
+ #endif
+ 1:
++	mcr	p15, 0, r0, c7, c10, 1		@ clean D / U line
++	add	r0, r0, r2
++	cmp	r0, r1
++	blo	1b
++	ret	lr
++ENDPROC(v7_dma_clean_range_no_dsb)
++
++/*
++ *  v7_dma_flush_range(start,end)
++ *	- start   - virtual start address of region
++ *	- end     - virtual end address of region
++ */
++ENTRY(v7_dma_flush_range)
++	dcache_line_size	r2, r3
++	sub	r3, r2, #1
++	bic	r0, r0, r3
++#ifdef	CONFIG_ARM_ERRATA_764369
++	ALT_SMP(W(dsb))
++	ALT_UP(W(nop))
++#endif
++1:
+ 	mcr	p15, 0, r0, c7, c14, 1		@ clean & invalidate D / U line
+ 	add	r0, r0, r2
+ 	cmp	r0, r1
+@@ -431,10 +492,35 @@ ENTRY(v7_dma_flush_range)
+ ENDPROC(v7_dma_flush_range)
+ 
+ /*
++ *  v7_dma_flush_range_no_dsb(start,end)
++ *
++ *	This API does not do "data synchronization barrier".
++ *	The caller is responsible to do dsb after the transaction.
++ *
++ *	- start   - virtual start address of region
++ *	- end     - virtual end address of region
++ */
++ENTRY(v7_dma_flush_range_no_dsb)
++	dcache_line_size	r2, r3
++	sub	r3, r2, #1
++	bic	r0, r0, r3
++#ifdef CONFIG_ARM_ERRATA_764369
++	ALT_SMP(W(dsb))
++	ALT_UP(W(nop))
++#endif
++1:
++	mcr	p15, 0, r0, c7, c14, 1		@ clean & invalidate D / U line
++	add	r0, r0, r2
++	cmp	r0, r1
++	blo	1b
++	ret	lr
++ENDPROC(v7_dma_flush_range_no_dsb)
++
++/*
+  *	dma_map_area(start, size, dir)
+- *	- start	- kernel virtual start address
+- *	- size	- size of region
+- *	- dir	- DMA direction
++ *	- start - kernel virtual start address
++ *	- size  - size of region
++ *	- dir   - DMA direction
+  */
+ ENTRY(v7_dma_map_area)
+ 	add	r1, r1, r0
+--- a/arch/arm/mm/proc-syms.c
++++ b/arch/arm/mm/proc-syms.c
+@@ -28,8 +28,11 @@ EXPORT_SYMBOL(__cpuc_flush_user_range);
+ EXPORT_SYMBOL(__cpuc_coherent_kern_range);
+ EXPORT_SYMBOL(__cpuc_flush_dcache_area);
+ EXPORT_SYMBOL(dmac_inv_range);
++EXPORT_SYMBOL(dmac_inv_range_no_dsb);
+ EXPORT_SYMBOL(dmac_clean_range);
++EXPORT_SYMBOL(dmac_clean_range_no_dsb);
+ EXPORT_SYMBOL(dmac_flush_range);
++EXPORT_SYMBOL(dmac_flush_range_no_dsb);
+ #else
+ EXPORT_SYMBOL(cpu_cache);
+ #endif
+--- a/arch/arm64/include/asm/assembler.h
++++ b/arch/arm64/include/asm/assembler.h
+@@ -428,6 +428,45 @@ alternative_endif
+ 
+ /*
+  * Macro to perform a data cache maintenance for the interval
++ *	[kaddr, kaddr + size)
++ *	This macro does not do "data synchronization barrier". Caller should
++ *	do "dsb" after transaction.
++ *
++ *	op:     operation passed to dc instruction
++ *	kaddr:      starting virtual address of the region
++ *	size:       size of the region
++ *	Corrupts:   kaddr, size, tmp1, tmp2
++ */
++	.macro dcache_by_line_op_no_dsb op, kaddr, size, tmp1, tmp2
++	dcache_line_size \tmp1, \tmp2
++	add \size, \kaddr, \size
++	sub \tmp2, \tmp1, #1
++	bic \kaddr, \kaddr, \tmp2
++9998:
++	.ifc    \op, cvau
++	__dcache_op_workaround_clean_cache \op, \kaddr
++	.else
++	.ifc	\op, cvac
++	__dcache_op_workaround_clean_cache \op, \kaddr
++	.else
++	.ifc	\op, cvap
++	sys	3, c7, c12, 1, \kaddr	// dc cvap
++	.else
++	.ifc	\op, cvadp
++	sys	3, c7, c13, 1, \kaddr	// dc cvadp
++	.else
++	dc	\op, \kaddr
++	.endif
++	.endif
++	.endif
++	.endif
++	add	\kaddr, \kaddr, \tmp1
++	cmp	\kaddr, \size
++	b.lo	9998b
++	.endm
++
++/*
++ * Macro to perform a data cache maintenance for the interval
+  * [start, end)
+  *
+  * 	op:		operation passed to dc instruction
+--- a/arch/arm64/include/asm/cacheflush.h
++++ b/arch/arm64/include/asm/cacheflush.h
+@@ -79,6 +79,15 @@ extern void dcache_clean_pou(unsigned lo
+ extern long caches_clean_inval_user_pou(unsigned long start, unsigned long end);
+ extern void sync_icache_aliases(unsigned long start, unsigned long end);
+ 
++extern void dmac_inv_range(const void *start, const void *end);
++extern void __dma_flush_area_no_dsb(const void *start, size_t size);
++extern void __dma_inv_area_no_dsb(const void *start, size_t size);
++extern void __dma_clean_area_no_dsb(const void *start, size_t size);
++
++extern void dmac_flush_range_no_dsb(const void *start, const void *end);
++extern void dmac_inv_range_no_dsb(const void *start, const void *end);
++extern void dmac_clean_range_no_dsb(const void *start, const void *end);
++
+ static inline void flush_icache_range(unsigned long start, unsigned long end)
+ {
+ 	caches_clean_inval_pou(start, end);
+--- a/arch/arm64/mm/cache.S
++++ b/arch/arm64/mm/cache.S
+@@ -164,6 +164,64 @@ SYM_FUNC_END(__pi_dcache_inval_poc)
+ SYM_FUNC_ALIAS(dcache_inval_poc, __pi_dcache_inval_poc)
+ 
+ /*
++ *  __dma_inv_area_no_dsb(start, size)
++ *
++ *	This macro does not do "data synchronization barrier". Caller should
++ *	do "dsb" after transaction.
++ *
++ *	 start   - virtual start address of region
++ *	 size    - size in question
++ */
++SYM_FUNC_START(__dma_inv_area_no_dsb)
++	add	x1, x1, x0
++	dcache_line_size	x2, x3
++	sub	x3, x2, #1
++	tst	x1, x3				// end cache line aligned?
++	bic	x1, x1, x3
++	b.eq	1f
++	dc	civac, x1			// clean & invalidate D / U line
++1:	tst	x0, x3				// start cache line aligned?
++	bic	x0, x0, x3
++	b.eq    2f
++	dc	civac, x0			// clean & invalidate D / U line
++	b	3f
++2:	dc  ivac, x0			// invalidate D / U line
++3:	add x0, x0, x2
++	cmp	x0, x1
++	b.lo	2b
++	ret
++SYM_FUNC_END(__dma_inv_area_no_dsb)
++
++/*
++ *  __dma_clean_area_no_dsb(start, size)
++ *
++ *	his macro does not do "data synchronization barrier". Caller should
++ *	o "dsb" after transaction.
++ *
++ *	 start   - virtual start address of region
++ *	 size    - size in question
++ */
++SYM_FUNC_START(__dma_clean_area_no_dsb)
++	dcache_by_line_op_no_dsb cvac, x0, x1, x2, x3
++	ret
++SYM_FUNC_END(__dma_clean_area_no_dsb)
++
++/*
++ *  __dma_flush_area_no_dsb(start, size)
++ *
++ *	clean & invalidate D / U line
++ *	his macro does not do "data synchronization barrier". Caller should
++ *	o "dsb" after transaction.
++ *
++ *	 start   - virtual start address of region
++ *	 size    - size in question
++ */
++SYM_FUNC_START(__dma_flush_area_no_dsb)
++	dcache_by_line_op_no_dsb civac, x0, x1, x2, x3
++	ret
++SYM_FUNC_END(__dma_flush_area_no_dsb)
++
++/*
+  *	dcache_clean_poc(start, end)
+  *
+  * 	Ensure that any D-cache lines for the interval [start, end)
+--- a/arch/arm64/mm/flush.c
++++ b/arch/arm64/mm/flush.c
+@@ -100,3 +100,27 @@ void arch_invalidate_pmem(void *addr, si
+ }
+ EXPORT_SYMBOL_GPL(arch_invalidate_pmem);
+ #endif
++
++void dmac_flush_range_no_dsb(const void *start, const void *end)
++{
++	__dma_flush_area_no_dsb(start, (void *)(end) - (void *)(start));
++}
++EXPORT_SYMBOL(dmac_flush_range_no_dsb);
++
++void dmac_inv_range(const void *start, const void *end)
++{
++	dcache_inval_poc((unsigned long)start, (unsigned long)(end));
++}
++EXPORT_SYMBOL(dmac_inv_range);
++
++void dmac_inv_range_no_dsb(const void *start, const void *end)
++{
++	__dma_inv_area_no_dsb(start, (void *)(end) - (void *)(start));
++}
++EXPORT_SYMBOL(dmac_inv_range_no_dsb);
++
++void dmac_clean_range_no_dsb(const void *start, const void *end)
++{
++	__dma_clean_area_no_dsb(start, (void *)(end) - (void *)(start));
++}
++EXPORT_SYMBOL(dmac_clean_range_no_dsb);
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950190-arm64-dma_mapping-Export-64bit-cache-functions.patch	2024-12-16 20:30:08.158889154 +0800
@@ -0,0 +1,57 @@
+From 2718af189011392e40c92237110dda1cd78f86d6 Mon Sep 17 00:00:00 2001
+From: Tushar Ganatra <quic_tganatra@quicinc.com>
+Date: Mon, 29 May 2023 17:10:49 +0530
+Subject: [PATCH 096/500] arm64: dma_mapping: Export 64bit cache functions
+
+Export dmac_ functions. Export dmac_* instead of __dma_*
+so that the same API can be used for both 32 and 64 bit builds.
+Change-Id: Ib84134b02ec5522089b7ee7bfbe7056c5e0fb2fd
+Signed-off-by: Selin Dag <sdag@codeaurora.org>
+Signed-off-by: Ratheesh Kannoth <rkannoth@codeaurora.org>
+Signed-off-by: Tushar Ganatra <quic_tganatra@quicinc.com>
+Signed-off-by: KARTHIK KUMAR T <quic_kartt@quicinc.com>
+---
+ arch/arm64/include/asm/cacheflush.h |  2 ++
+ arch/arm64/mm/flush.c               | 12 ++++++++++++
+ 2 files changed, 14 insertions(+)
+
+--- a/arch/arm64/include/asm/cacheflush.h
++++ b/arch/arm64/include/asm/cacheflush.h
+@@ -79,7 +79,9 @@ extern void dcache_clean_pou(unsigned lo
+ extern long caches_clean_inval_user_pou(unsigned long start, unsigned long end);
+ extern void sync_icache_aliases(unsigned long start, unsigned long end);
+ 
++extern void dmac_flush_range(const void *start, const void *end);
+ extern void dmac_inv_range(const void *start, const void *end);
++extern void dmac_clean_range(const void *start, const void *end);
+ extern void __dma_flush_area_no_dsb(const void *start, size_t size);
+ extern void __dma_inv_area_no_dsb(const void *start, size_t size);
+ extern void __dma_clean_area_no_dsb(const void *start, size_t size);
+--- a/arch/arm64/mm/flush.c
++++ b/arch/arm64/mm/flush.c
+@@ -101,6 +101,12 @@ void arch_invalidate_pmem(void *addr, si
+ EXPORT_SYMBOL_GPL(arch_invalidate_pmem);
+ #endif
+ 
++void dmac_flush_range(const void *start, const void *end)
++{
++	dcache_clean_inval_poc((unsigned long)start, (unsigned long)end);
++}
++EXPORT_SYMBOL(dmac_flush_range);
++
+ void dmac_flush_range_no_dsb(const void *start, const void *end)
+ {
+ 	__dma_flush_area_no_dsb(start, (void *)(end) - (void *)(start));
+@@ -119,6 +125,12 @@ void dmac_inv_range_no_dsb(const void *s
+ }
+ EXPORT_SYMBOL(dmac_inv_range_no_dsb);
+ 
++void dmac_clean_range(const void *start, const void *end)
++{
++      dcache_clean_poc((unsigned long)start, (unsigned long)end);
++}
++EXPORT_SYMBOL(dmac_clean_range);
++
+ void dmac_clean_range_no_dsb(const void *start, const void *end)
+ {
+ 	__dma_clean_area_no_dsb(start, (void *)(end) - (void *)(start));
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950277-net-Bypass-RPS-hash-calculation-for-a-single-CPU-map.patch	2024-12-16 20:31:55.035160418 +0800
@@ -0,0 +1,38 @@
+From 71f70da4bc8df8b967089cacf7dc7376497732ee Mon Sep 17 00:00:00 2001
+From: KRITHI D SHETTY <quic_kdshetty@quicinc.com>
+Date: Thu, 7 Sep 2023 10:42:12 +0530
+Subject: [PATCH 172/500] net: Bypass RPS hash calculation for a single CPU
+ map.
+
+Change-Id: Iab225d8b7b0c22e57ec1ec90cd643d1ee387ceab
+Signed-off-by: KRITHI D SHETTY <quic_kdshetty@quicinc.com>
+---
+ net/core/dev.c | 17 +++++++++++++++--
+ 1 file changed, 15 insertions(+), 2 deletions(-)
+
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -4700,8 +4700,21 @@ static int get_rps_cpu(struct net_device
+ 
+ 	flow_table = rcu_dereference(rxqueue->rps_flow_table);
+ 	map = rcu_dereference(rxqueue->rps_map);
+-	if (!flow_table && !map)
+-		goto done;
++
++	if (!flow_table) {
++		if (!map) {
++			goto done;
++		}
++
++		/* Skip hash calculation & lookup if we have only one CPU to transmit and RFS is disabled */
++		if (map->len == 1) {
++			tcpu = map->cpus[0];
++			if (cpu_online(tcpu)) {
++				cpu = tcpu;
++				goto done;
++			}
++		}
++	}
+ 
+ 	skb_reset_network_header(skb);
+ 	hash = skb_get_hash(skb);
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950287-net-prefetch-skb-while-dequeuing-from-backlog-list.patch	2024-12-16 20:31:55.158158432 +0800
@@ -0,0 +1,20 @@
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -6140,10 +6140,16 @@ static int process_backlog(struct napi_s
+ 
+ 	napi->weight = READ_ONCE(dev_rx_weight);
+ 	while (again) {
+-		struct sk_buff *skb;
++		struct sk_buff *skb, *next_skb;
+ 
+ 		while ((skb = __skb_dequeue(&sd->process_queue))) {
+ 			rcu_read_lock();
++
++			next_skb = skb_peek(&sd->process_queue);
++			if (likely(next_skb)) {
++				prefetch(next_skb->data);
++			}
++
+ 			__netif_receive_skb(skb);
+ 			rcu_read_unlock();
+ 			input_queue_head_incr(sd);
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950303-linux-Avoid-invalidate-if-recycled_for_ds-flag-is-se.patch	2024-12-16 20:30:08.163889073 +0800
@@ -0,0 +1,25 @@
+From ddb13d29c6289c8516da19e16e8fa012956cc849 Mon Sep 17 00:00:00 2001
+From: Nanda Krishnan <quic_nandkris@quicinc.com>
+Date: Thu, 13 Oct 2022 11:54:20 +0530
+Subject: [PATCH 249/500] [linux] Avoid invalidate if recycled_for_ds flag is
+ set
+
+Avoiding invalidation in wifi if recycled_for_ds flag is set
+
+Change-Id: I1841d821597a27833437c203c0adc2dc4ba0001a
+Signed-off-by: Nanda Krishnan <quic_nandkris@quicinc.com>
+---
+ include/linux/skbuff.h | 2 ++
+ 1 file changed, 2 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -986,6 +986,8 @@ struct sk_buff {
+ #endif
+ 	/* Linear packets processed by dev_fast_xmit() */
+ 	__u8			fast_xmit:1;
++	/* Flag for recycle in PPE DS */
++	__u8			recycled_for_ds:1;
+ 
+ #if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)
+ 	__u16			tc_index;	/* traffic control index */
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950304-skb_recycler-Add-flags-in-skb.patch	2024-12-16 20:31:55.394154622 +0800
@@ -0,0 +1,27 @@
+From a44bac564481480f66f096301c6f281b0e3b9228 Mon Sep 17 00:00:00 2001
+From: Neha Bisht <quic_nbisht@quicinc.com>
+Date: Thu, 6 Oct 2022 14:37:49 +0530
+Subject: [PATCH 250/500] skb_recycler: Add flags in skb
+
+Add flags in skb for checking if skb is fast recycled and
+packet is coming from recycler
+
+Change-Id: Iea91f75a7989799da71006b4609ba7326c9e07cd
+Signed-off-by: Neha Bisht <quic_nbisht@quicinc.com>
+---
+ include/linux/skbuff.h | 4 ++++
+ 1 file changed, 4 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -986,6 +986,10 @@ struct sk_buff {
+ #endif
+ 	/* Linear packets processed by dev_fast_xmit() */
+ 	__u8			fast_xmit:1;
++	/* Flag to check if skb is allocated from recycler */
++	__u8			is_from_recycler:1;
++	/* Flag for fast recycle in fast xmit path */
++	__u8			fast_recycled:1;
+ 	/* Flag for recycle in PPE DS */
+ 	__u8			recycled_for_ds:1;
+ 
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950305-skb_recycler-Add-an-API-to-free-skb-recycler-list.patch	2024-12-16 20:31:55.520152588 +0800
@@ -0,0 +1,84 @@
+From 59785fdb827ec81d68adaddff6b75ad77ff175c3 Mon Sep 17 00:00:00 2001
+From: Neha Bisht <quic_nbisht@quicinc.com>
+Date: Mon, 18 Jul 2022 17:09:00 +0530
+Subject: [PATCH 251/500] skb_recycler: Add an API to free skb recycler list
+
+Currently we have consume_skb which takes a single skb buffer
+as an input and process on them one by one. To avoid this processing
+and local_irq calls, we add an API to take a list of skbs as an input
+to free them at once to save CPU cycles.
+Also, we have avoided global and spare list processing here by assuming
+hot skb recycle list is always available.
+
+Change-Id: I5c99b9281ec6d7ab189da4e8d30c6cb8a8817d7c
+Signed-off-by: Neha Bisht <quic_nbisht@quicinc.com>
+---
+ include/linux/skbuff.h |  2 ++
+ net/core/skbuff.c      | 38 ++++++++++++++++++++++++++++++++++++++
+ 2 files changed, 40 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1257,6 +1257,7 @@ static inline void consume_skb(struct sk
+ }
+ #endif
+ 
++void consume_skb_list_fast(struct sk_buff_head *skb_list);
+ void __consume_stateless_skb(struct sk_buff *skb);
+ void  __kfree_skb(struct sk_buff *skb);
+ extern struct kmem_cache *skbuff_cache;
+@@ -1382,6 +1383,7 @@ static inline int skb_pad(struct sk_buff
+ 	return __skb_pad(skb, pad, true);
+ }
+ #define dev_kfree_skb(a)	consume_skb(a)
++#define dev_kfree_skb_list_fast(a)	consume_skb_list_fast(a)
+ 
+ int skb_append_pagefrags(struct sk_buff *skb, struct page *page,
+ 			 int offset, size_t size, size_t max_frags);
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -1303,6 +1303,44 @@ EXPORT_SYMBOL(consume_skb);
+ #endif
+ 
+ /**
++ *	consume_skb_list_fast - free a list of skbs
++ *	@skb_list: head of the buffer list
++ *
++ *	Add the list of given SKBs to CPU list. Assumption is that these buffers
++ *	have been allocated originally from the skb recycler and have been transmitted
++ *	through a controlled fast xmit path, thus removing the need for additional checks
++ *	before recycling the buffers back to pool
++ */
++void consume_skb_list_fast(struct sk_buff_head *skb_list)
++{
++	struct sk_buff *skb = NULL;
++
++	if (likely(skb_recycler_consume_list_fast(skb_list))) {
++		return;
++	}
++
++	while ((skb = skb_dequeue(skb_list)) != NULL) {
++		/*
++		 * Check if release head state is needed
++		 */
++		skb_release_head_state(skb);
++
++		trace_consume_skb(skb, __builtin_return_address(0));
++
++		/*
++		 * We're not recycling so now we need to do the rest of what we would
++		 * have done in __kfree_skb (above and beyond the skb_release_head_state
++		 * that we already did).
++		 */
++		if (likely(skb->head))
++			skb_release_data(skb, SKB_CONSUMED, false);
++
++		kfree_skbmem(skb);
++	}
++}
++EXPORT_SYMBOL(consume_skb_list_fast);
++
++/**
+  *	__consume_stateless_skb - free an skbuff, assuming it is stateless
+  *	@skb: buffer to free
+  *
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950306-skb_recycler-Add-debug-code-to-check-if-skb-is-recyc.patch	2024-12-16 20:31:55.646150553 +0800
@@ -0,0 +1,32 @@
+From 8d9bba96d1297e2f6135b523bfa16be01e41b1d3 Mon Sep 17 00:00:00 2001
+From: Neha Bisht <quic_nbisht@quicinc.com>
+Date: Mon, 19 Sep 2022 16:50:49 +0530
+Subject: [PATCH 252/500] skb_recycler: Add debug code to check if skb is
+ recyclable.
+
+Add debug code to check for various conditions on skb recyclability.
+
+Change-Id: I073d90da38f9354cd7c15dcff64b404162d8b01c
+Signed-off-by: Neha Bisht <quic_nbisht@quicinc.com>
+---
+ include/linux/skbuff.h | 2 ++
+ 1 file changed, 2 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1258,6 +1258,7 @@ static inline void consume_skb(struct sk
+ #endif
+ 
+ void consume_skb_list_fast(struct sk_buff_head *skb_list);
++void check_skb_fast_recyclable(struct sk_buff *skb);
+ void __consume_stateless_skb(struct sk_buff *skb);
+ void  __kfree_skb(struct sk_buff *skb);
+ extern struct kmem_cache *skbuff_cache;
+@@ -1384,6 +1385,7 @@ static inline int skb_pad(struct sk_buff
+ }
+ #define dev_kfree_skb(a)	consume_skb(a)
+ #define dev_kfree_skb_list_fast(a)	consume_skb_list_fast(a)
++#define dev_check_skb_fast_recyclable(a)       check_skb_fast_recyclable(a)
+ 
+ int skb_append_pagefrags(struct sk_buff *skb, struct page *page,
+ 			 int offset, size_t size, size_t max_frags);
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950307-skb_recycler-Enable-reycler-debug-api-under-appropri.patch	2024-12-16 20:31:55.765148632 +0800
@@ -0,0 +1,29 @@
+From c39c2357e360dd7e70d89885483677783a5ef20a Mon Sep 17 00:00:00 2001
+From: Neha Bisht <quic_nbisht@quicinc.com>
+Date: Tue, 24 Jan 2023 13:21:24 +0530
+Subject: [PATCH 253/500] skb_recycler: Enable reycler debug api under
+ appropriate macro
+
+Enable skb fast recycler debug api under reycler and debug check
+macros
+
+Change-Id: I0386ef6391761d2804f06b29b1d886b8ed0d117a
+Signed-off-by: Neha Bisht <quic_nbisht@quicinc.com>
+---
+ include/linux/skbuff.h | 4 ++++
+ 1 file changed, 4 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1385,7 +1385,11 @@ static inline int skb_pad(struct sk_buff
+ }
+ #define dev_kfree_skb(a)	consume_skb(a)
+ #define dev_kfree_skb_list_fast(a)	consume_skb_list_fast(a)
++#if defined(SKB_FAST_RECYCLABLE_DEBUG_ENABLE) && defined(CONFIG_SKB_RECYCLER)
+ #define dev_check_skb_fast_recyclable(a)       check_skb_fast_recyclable(a)
++#else
++#define dev_check_skb_fast_recyclable(a)
++#endif
+ 
+ int skb_append_pagefrags(struct sk_buff *skb, struct page *page,
+ 			 int offset, size_t size, size_t max_frags);
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950308-net-skbuff-cache-for-skb-data-for-LM-profiles.patch	2024-12-16 20:30:08.170888960 +0800
@@ -0,0 +1,70 @@
+From 9934de94dcd85d483097ac0a0c6a2ef078a08d25 Mon Sep 17 00:00:00 2001
+From: Kathiravan T <kathirav@codeaurora.org>
+Date: Thu, 29 Apr 2021 16:29:02 +0530
+Subject: [PATCH 254/500] net: skbuff: cache for skb->data for LM profiles
+
+Signed-off-by: Kathiravan T <kathirav@codeaurora.org>
+Change-Id: I7bcac8d05a11aa3d1a9e015d6530562ed81477a0
+---
+ net/core/skbuff.c | 29 +++++++++++++++++++++++++++--
+ 1 file changed, 27 insertions(+), 2 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -89,6 +89,12 @@
+ #include "sock_destructor.h"
+ 
+ struct kmem_cache *skbuff_cache __ro_after_init;
++
++#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
++struct kmem_cache *skb_data_cache;
++#define SKB_DATA_CACHE_SIZE	2176
++#endif
++
+ static struct kmem_cache *skbuff_fclone_cache __ro_after_init;
+ #ifdef CONFIG_SKB_EXTENSIONS
+ static struct kmem_cache *skbuff_ext_cache __ro_after_init;
+@@ -579,7 +585,14 @@ static void *kmalloc_reserve(unsigned in
+ 	 * Try a regular allocation, when that fails and we're not entitled
+ 	 * to the reserves, fail.
+ 	 */
+-	obj = kmalloc_node_track_caller(obj_size,
++#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
++	if (size > SZ_2K && size <= SKB_DATA_CACHE_SIZE)
++		obj = kmem_cache_alloc_node(skb_data_cache,
++						flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
++						node);
++	else
++#endif
++		obj = kmalloc_node_track_caller(obj_size,
+ 					flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+ 					node);
+ 	if (obj || !(gfp_pfmemalloc_allowed(flags)))
+@@ -587,7 +600,12 @@ static void *kmalloc_reserve(unsigned in
+ 
+ 	/* Try again but now we are using pfmemalloc reserves */
+ 	ret_pfmemalloc = true;
+-	obj = kmalloc_node_track_caller(obj_size, flags, node);
++#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
++	if (size > SZ_2K && size <= SKB_DATA_CACHE_SIZE)
++		obj = kmem_cache_alloc_node(skb_data_cache, flags, node);
++	else
++#endif
++		obj = kmalloc_node_track_caller(obj_size, flags, node);
+ 
+ out:
+ 	if (pfmemalloc)
+@@ -4903,6 +4921,13 @@ static void skb_extensions_init(void) {}
+ 
+ void __init skb_init(void)
+ {
++#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
++	skb_data_cache = kmem_cache_create_usercopy("skb_data_cache",
++						SKB_DATA_CACHE_SIZE,
++						0, 0, 0, SKB_DATA_CACHE_SIZE,
++						NULL);
++#endif
++
+ 	skbuff_cache = kmem_cache_create_usercopy("skbuff_head_cache",
+ 					      sizeof(struct sk_buff),
+ 					      0,
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950309-net-skbuff-Cache-for-skb-data-for-all-profiles.patch	2024-12-16 20:30:08.171888943 +0800
@@ -0,0 +1,91 @@
+From c5be7baf8d915e91e6f133edd3fae786fa06d487 Mon Sep 17 00:00:00 2001
+From: Sneha Maganahalli <quic_smaganah@quicinc.com>
+Date: Fri, 4 Feb 2022 14:29:37 +0530
+Subject: [PATCH 255/500] net: skbuff: Cache for skb->data for all profiles.
+
+Cache for skb->data for all profiles.
+
+Change-Id: Ib84a1b0c037bc27febb24edbee91ffe6427d528d
+Signed-off-by: Sneha Maganahalli <quic_smaganah@quicinc.com>
+---
+ net/core/skbuff.c | 34 +++++++++++++++++++++++++---------
+ 1 file changed, 25 insertions(+), 9 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -88,11 +88,32 @@
+ #include "dev.h"
+ #include "sock_destructor.h"
+ 
++
+ struct kmem_cache *skbuff_cache __ro_after_init;
+ 
+-#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
+ struct kmem_cache *skb_data_cache;
+-#define SKB_DATA_CACHE_SIZE	2176
++
++/*
++ * For low memory profile, NSS_SKB_FIXED_SIZE_2K is enabled and
++ * CONFIG_SKB_RECYCLER is disabled. For premium and enterprise profile
++ * CONFIG_SKB_RECYCLER is enabled and NSS_SKB_FIXED_SIZE_2K is disabled.
++ * Irrespective of NSS_SKB_FIXED_SIZE_2K enabled/disabled, the
++ * CONFIG_SKB_RECYCLER and __LP64__ determines the value of SKB_DATA_CACHE_SIZE
++ */
++#if defined(CONFIG_SKB_RECYCLER)
++/*
++ * 2688 for 64bit arch, 2624 for 32bit arch
++ */
++#define SKB_DATA_CACHE_SIZE (SKB_DATA_ALIGN(SKB_RECYCLE_SIZE + NET_SKB_PAD) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++#else
++/*
++ * 2368 for 64bit arch, 2176 for 32bit arch
++ */
++#if defined(__LP64__)
++#define SKB_DATA_CACHE_SIZE ((SKB_DATA_ALIGN(1984 + NET_SKB_PAD)) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++#else
++#define SKB_DATA_CACHE_SIZE ((SKB_DATA_ALIGN(1856 + NET_SKB_PAD)) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++#endif
+ #endif
+ 
+ static struct kmem_cache *skbuff_fclone_cache __ro_after_init;
+@@ -585,13 +606,11 @@ static void *kmalloc_reserve(unsigned in
+ 	 * Try a regular allocation, when that fails and we're not entitled
+ 	 * to the reserves, fail.
+ 	 */
+-#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
+ 	if (size > SZ_2K && size <= SKB_DATA_CACHE_SIZE)
+ 		obj = kmem_cache_alloc_node(skb_data_cache,
+ 						flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+ 						node);
+ 	else
+-#endif
+ 		obj = kmalloc_node_track_caller(obj_size,
+ 					flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+ 					node);
+@@ -600,11 +619,9 @@ static void *kmalloc_reserve(unsigned in
+ 
+ 	/* Try again but now we are using pfmemalloc reserves */
+ 	ret_pfmemalloc = true;
+-#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
+ 	if (size > SZ_2K && size <= SKB_DATA_CACHE_SIZE)
+ 		obj = kmem_cache_alloc_node(skb_data_cache, flags, node);
+ 	else
+-#endif
+ 		obj = kmalloc_node_track_caller(obj_size, flags, node);
+ 
+ out:
+@@ -4921,12 +4938,11 @@ static void skb_extensions_init(void) {}
+ 
+ void __init skb_init(void)
+ {
+-#if defined(CONFIG_SKB_FIXED_SIZE_2K) && !defined(__LP64__)
++
+ 	skb_data_cache = kmem_cache_create_usercopy("skb_data_cache",
+ 						SKB_DATA_CACHE_SIZE,
+-						0, 0, 0, SKB_DATA_CACHE_SIZE,
++						0, SLAB_PANIC, 0, SKB_DATA_CACHE_SIZE,
+ 						NULL);
+-#endif
+ 
+ 	skbuff_cache = kmem_cache_create_usercopy("skbuff_head_cache",
+ 					      sizeof(struct sk_buff),
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950310-Generic-rx-to-rx-skb-recycler.patch	2024-12-16 20:31:56.137142627 +0800
@@ -0,0 +1,208 @@
+From d307242214f950f7e961f540ad21a00ed331c6f4 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Mon, 13 Apr 2020 10:14:08 -0700
+Subject: [PATCH 256/500] Generic rx-to-rx skb recycler
+
+Porting from 3.4 kernel (banana branch).
+
+Removing kmemcheck header and the no longer valid cpuhotplug notifier function.
+
+Change-Id: Ie81a7d812ec14a40da8f22cf4e0f7ddaaf166cff
+Signed-off-by: Varadarajan Narayanan <varada@codeaurora.org>
+Signed-off-by: Pamidipati, Vijay <vpamidip@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+Signed-off-by: Tian Yang <tiany@codeaurora.org>
+---
+ MAINTAINERS            |  5 ++++
+ include/linux/skbuff.h |  3 ++
+ net/Kconfig            | 15 ++++++++++
+ net/core/Makefile      |  1 +
+ net/core/skbuff.c      | 68 +++++++++++++++++++++++++++++++++++++-----
+ 5 files changed, 85 insertions(+), 7 deletions(-)
+
+--- a/MAINTAINERS
++++ b/MAINTAINERS
+@@ -67,6 +67,11 @@ Maintainers List
+           first. When adding to this list, please keep the entries in
+           alphabetical order.
+ 
++SKB RECYCLER SUPPORT
++M:	Casey Chen <kexinc@codeaurora.org>
++S:	Maintained
++F:	net/core/skbuff_recycle.*
++
+ 3C59X NETWORK DRIVER
+ M:	Steffen Klassert <klassert@kernel.org>
+ L:	netdev@vger.kernel.org
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1262,6 +1262,9 @@ void check_skb_fast_recyclable(struct sk
+ void __consume_stateless_skb(struct sk_buff *skb);
+ void  __kfree_skb(struct sk_buff *skb);
+ extern struct kmem_cache *skbuff_cache;
++extern void kfree_skbmem(struct sk_buff *skb);
++extern void skb_release_data(struct sk_buff *skb, enum skb_drop_reason reason,
++			     bool napi_safe);
+ 
+ void kfree_skb_partial(struct sk_buff *skb, bool head_stolen);
+ bool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,
+--- a/net/Kconfig
++++ b/net/Kconfig
+@@ -369,6 +369,21 @@ config NET_FLOW_LIMIT
+ 	  with many clients some protection against DoS by a single (spoofed)
+ 	  flow that greatly exceeds average workload.
+ 
++config SKB_RECYCLER
++	bool "Generic skb recycling"
++	default y
++	help
++	  SKB_RECYCLER is used to implement RX-to-RX skb recycling.
++	  This config enables the recycling scheme for bridging and
++	  routing workloads. It can reduce skbuff freeing or
++	  reallocation overhead.
++
++
++config SKB_RECYCLER_MULTI_CPU
++	bool "Cross-CPU recycling for CPU-locked workloads"
++	depends on SKB_RECYCLER
++	default y
++
+ menu "Network testing"
+ 
+ config NET_PKTGEN
+--- a/net/core/Makefile
++++ b/net/core/Makefile
+@@ -41,3 +41,4 @@ obj-$(CONFIG_NET_SOCK_MSG) += skmsg.o
+ obj-$(CONFIG_BPF_SYSCALL) += sock_map.o
+ obj-$(CONFIG_BPF_SYSCALL) += bpf_sk_storage.o
+ obj-$(CONFIG_OF)	+= of_net.o
++obj-$(CONFIG_SKB_RECYCLER) += skbuff_recycle.o
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -116,6 +116,8 @@ struct kmem_cache *skb_data_cache;
+ #endif
+ #endif
+ 
++#include "skbuff_recycle.h"
++
+ static struct kmem_cache *skbuff_fclone_cache __ro_after_init;
+ #ifdef CONFIG_SKB_EXTENSIONS
+ static struct kmem_cache *skbuff_ext_cache __ro_after_init;
+@@ -721,7 +723,7 @@ EXPORT_SYMBOL(__alloc_skb);
+ /**
+  *	__netdev_alloc_skb - allocate an skbuff for rx on a specific device
+  *	@dev: network device to receive on
+- *	@len: length to allocate
++ *	@length: length to allocate
+  *	@gfp_mask: get_free_pages mask, passed to alloc_skb
+  *
+  *	Allocate a new &sk_buff and assign it a usage count of one. The
+@@ -731,11 +733,28 @@ EXPORT_SYMBOL(__alloc_skb);
+  *
+  *	%NULL is returned if there is no free memory.
+  */
+-struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,
+-				   gfp_t gfp_mask)
++struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
++				   unsigned int length, gfp_t gfp_mask)
+ {
+-	struct page_frag_cache *nc;
+ 	struct sk_buff *skb;
++	unsigned int len = length;
++
++#ifdef CONFIG_SKB_RECYCLER
++	skb = skb_recycler_alloc(dev, length);
++	if (likely(skb))
++		return skb;
++
++	len = SKB_RECYCLE_SIZE;
++	if (unlikely(length > SKB_RECYCLE_SIZE))
++		len = length;
++
++	skb = __alloc_skb(len + NET_SKB_PAD, gfp_mask,
++			  SKB_ALLOC_RX, NUMA_NO_NODE);
++	if (!skb)
++		goto skb_fail;
++	goto skb_success;
++#else
++	struct page_frag_cache *nc;
+ 	bool pfmemalloc;
+ 	void *data;
+ 
+@@ -782,6 +801,7 @@ struct sk_buff *__netdev_alloc_skb(struc
+ 	if (pfmemalloc)
+ 		skb->pfmemalloc = 1;
+ 	skb->head_frag = 1;
++#endif
+ 
+ skb_success:
+ 	skb_reserve(skb, NET_SKB_PAD);
+@@ -1010,7 +1030,7 @@ static void skb_free_head(struct sk_buff
+ 	}
+ }
+ 
+-static void skb_release_data(struct sk_buff *skb, enum skb_drop_reason reason,
++void skb_release_data(struct sk_buff *skb, enum skb_drop_reason reason,
+ 			     bool napi_safe)
+ {
+ 	struct skb_shared_info *shinfo = skb_shinfo(skb);
+@@ -1053,7 +1073,7 @@ exit:
+ /*
+  *	Free an skbuff by memory without cleaning the state.
+  */
+-static void kfree_skbmem(struct sk_buff *skb)
++void kfree_skbmem(struct sk_buff *skb)
+ {
+ 	struct sk_buff_fclones *fclones;
+ 
+@@ -1331,8 +1351,41 @@ void consume_skb(struct sk_buff *skb)
+ 	if (!skb_unref(skb))
+ 		return;
+ 
++	prefetch(&skb->destructor);
++
++	/*Tian: Not sure if we need to continue using this since
++	 * since unref does the work in 5.4
++	 */
++
++	/*
++	if (likely(atomic_read(&skb->users) == 1))
++		smp_rmb();
++	else if (likely(!atomic_dec_and_test(&skb->users)))
++		return;
++	*/
++
++	/* If possible we'd like to recycle any skb rather than just free it,
++	 * but in order to do that we need to release any head state too.
++	 * We don't want to do this later because we'll be in a pre-emption
++	 * disabled state.
++	 */
++	skb_release_head_state(skb);
++
++	/* Can we recycle this skb?  If we can then it will be much faster
++	 * for us to recycle this one later than to allocate a new one
++	 * from scratch.
++	 */
++	if (likely(skb_recycler_consume(skb)))
++		return;
++
+ 	trace_consume_skb(skb, __builtin_return_address(0));
+-	__kfree_skb(skb);
++
++	/* We're not recycling so now we need to do the rest of what we would
++	 * have done in __kfree_skb (above and beyond the skb_release_head_state
++	 * that we already did).
++	 */
++	skb_release_data(skb, SKB_CONSUMED, false);
++	kfree_skbmem(skb);
+ }
+ EXPORT_SYMBOL(consume_skb);
+ #endif
+@@ -4969,6 +5022,7 @@ void __init skb_init(void)
+ 						SKB_SMALL_HEAD_HEADROOM,
+ 						NULL);
+ 	skb_extensions_init();
++	skb_recycler_init();
+ }
+ 
+ static int
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950311-Support-preallocation-of-skbs.patch	2024-12-16 20:31:56.256140707 +0800
@@ -0,0 +1,43 @@
+From 3fb282977fa4aeb1d23a8a07b4ebd89d804db059 Mon Sep 17 00:00:00 2001
+From: Vivek Natarajan <nataraja@codeaurora.org>
+Date: Thu, 8 Oct 2015 12:57:51 +0530
+Subject: [PATCH 257/500] Support preallocation of skbs
+
+This patch preallocates SKBs each of 4K size in recycling lists.
+The number of SKBs are configured through
+CONFIG_SKB_RECYCLE_MAX_PREALLOC_SKBS.
+
+Change-Id: I581813dfb298da2844aca86bebf9d80399d629ed
+Signed-off-by: Vivek Natarajan <nataraja@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+---
+ net/Kconfig | 18 ++++++++++++++++++
+ 1 file changed, 18 insertions(+)
+
+--- a/net/Kconfig
++++ b/net/Kconfig
+@@ -384,6 +384,24 @@ config SKB_RECYCLER_MULTI_CPU
+ 	depends on SKB_RECYCLER
+ 	default y
+ 
++config SKB_RECYCLER_PREALLOC
++	bool "Enable preallocation of SKBs"
++	depends on SKB_RECYCLER
++	default n
++	help
++	 Preallocates SKBs in recycling lists and the number of
++	 SKBs are configured through CONFIG_SKB_RECYCLE_MAX_PREALLOC_SKBS.
++	 This needs SKB_RECYCLER to be enabled.
++	 The number of preallocated SKBs can be passed using
++	 SKB_RECYCLE_MAX_PREALLOC_SKBS.
++
++config SKB_RECYCLE_MAX_PREALLOC_SKBS
++	int "Number of SKBs to be preallocated"
++	depends on SKB_RECYCLER_PREALLOC
++	default 16384
++	help
++	 Number of SKBs each of 4K size to be preallocated for recycling
++
+ menu "Network testing"
+ 
+ config NET_PKTGEN
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950312-skb_recycler-Add-a-cpustate-for-skb_recycler.patch	2024-12-16 20:30:08.175888879 +0800
@@ -0,0 +1,25 @@
+From 562049bde084714bb7c896237391ad5d84ceb83b Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Wed, 29 Jul 2020 17:39:28 -0700
+Subject: [PATCH 259/500] skb_recycler: Add a cpustate for skb_recycler
+
+Add one cpu hotplug state for skb_recycler, called CPUHP_SKB_RECYCLER_DEAD
+to avoid using NET_DEV state, this solves a warning calltrace issue from net_dev
+since it cannot register its NET_DEV_CPU_DEAD callback during its initialization.
+
+Signed-off-by: Tian Yang <tiany@codeaurora.org>
+Change-Id: I6f5729ee300248ade42317114847959fda42dd20
+---
+ include/linux/cpuhotplug.h | 1 +
+ 1 file changed, 1 insertion(+)
+
+--- a/include/linux/cpuhotplug.h
++++ b/include/linux/cpuhotplug.h
+@@ -94,6 +94,7 @@ enum cpuhp_state {
+ 	CPUHP_RADIX_DEAD,
+ 	CPUHP_PAGE_ALLOC,
+ 	CPUHP_NET_DEV_DEAD,
++	CPUHP_SKB_RECYCLER_DEAD,
+ 	CPUHP_PCI_XGENE_DEAD,
+ 	CPUHP_IOMMU_IOVA_DEAD,
+ 	CPUHP_LUSTRE_CFS_DEAD,
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950313-skbuff_recycler-Memzero-avoidance-in-alloc-for-DS.patch	2024-12-16 20:31:56.496136833 +0800
@@ -0,0 +1,135 @@
+From b9dbce93dae49be8db1e2711795dfc7088c9d483 Mon Sep 17 00:00:00 2001
+From: Nanda Krishnan <quic_nandkris@quicinc.com>
+Date: Tue, 15 Nov 2022 12:18:38 +0530
+Subject: [PATCH 260/500] skbuff_recycler: Memzero avoidance in alloc for DS
+
+Requirement:
+
+In skb recycler, if recyler module allocates the buffers
+already used by DS module to DS, then memzero, shinfo
+reset can be avoided, since the DS packets were not
+processed by SW (host).
+Hence, we will acheive good KPI with less CPU
+utlization.
+
+Fix:
+1) Introduced an API __netdev_alloc_skb_no_skb_reset for DS
+module.
+2) Added reset_skb flag as argument in skb_recycler_alloc
+to identify whether memzero, shinfo reset can be avoided
+or not.
+
+Change-Id: Ib7dc5f49775b6e35ac778ecf75cfecc601cee7b6
+Signed-off-by: Nanda Krishnan <quic_nandkris@quicinc.com>
+---
+ include/linux/skbuff.h |  3 ++
+ net/core/skbuff.c      | 78 ++++++++++++++++++++++++++++++++++++++++--
+ 2 files changed, 79 insertions(+), 2 deletions(-)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -3250,6 +3250,9 @@ static inline void *netdev_alloc_frag_al
+ struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,
+ 				   gfp_t gfp_mask);
+ 
++struct sk_buff *__netdev_alloc_skb_no_skb_reset(struct net_device *dev, unsigned int length,
++				   gfp_t gfp_mask);
++
+ /**
+  *	netdev_alloc_skb - allocate an skbuff for rx on a specific device
+  *	@dev: network device to receive on
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -740,9 +740,17 @@ struct sk_buff *__netdev_alloc_skb(struc
+ 	unsigned int len = length;
+ 
+ #ifdef CONFIG_SKB_RECYCLER
+-	skb = skb_recycler_alloc(dev, length);
+-	if (likely(skb))
++	bool reset_skb = true;
++	skb = skb_recycler_alloc(dev, length, reset_skb);
++	if (likely(skb)) {
++		/* SKBs in the recycler are from various unknown sources.
++		* Their truesize is unknown. We should set truesize
++		* as the needed buffer size before using it.
++		*/
++		skb->truesize = SKB_TRUESIZE(SKB_DATA_ALIGN(len + NET_SKB_PAD));
++		skb->recycled_for_ds = 0;
+ 		return skb;
++	}
+ 
+ 	len = SKB_RECYCLE_SIZE;
+ 	if (unlikely(length > SKB_RECYCLE_SIZE))
+@@ -812,6 +820,72 @@ skb_fail:
+ }
+ EXPORT_SYMBOL(__netdev_alloc_skb);
+ 
++#ifdef CONFIG_SKB_RECYCLER
++/* __netdev_alloc_skb_no_skb_reset - allocate an skbuff for rx on a specific device
++ *	@dev: network device to receive on
++ *	@length: length to allocate
++ *	@gfp_mask: get_free_pages mask, passed from wifi driver
++ *
++ *	Allocate a new &sk_buff and assign it a usage count of one. The
++ *	buffer has NET_SKB_PAD headroom built in. Users should allocate
++ *	the headroom they think they need without accounting for the
++ *	built in space. The built in space is used for optimisations.
++ *
++ *	Currently, using __netdev_alloc_skb_no_skb_reset for DS alone
++ *	and it invokes skb_recycler_alloc with reset_skb as false.
++ *	Hence, recycler pool will not do reset_struct when it
++ *	allocates DS used buffer to DS module, which will
++ *	improve the performance
++ *
++ *      %NULL is returned if there is no free memory.
++ */
++struct sk_buff *__netdev_alloc_skb_no_skb_reset(struct net_device *dev,
++						unsigned int length, gfp_t gfp_mask)
++{
++	struct sk_buff *skb;
++	unsigned int len = length;
++	bool reset_skb = false;
++
++	skb = skb_recycler_alloc(dev, length, reset_skb);
++	if (likely(skb)) {
++		/* SKBs in the recycler are from various unknown sources.
++		* Their truesize is unknown. We should set truesize
++		* as the needed buffer size before using it.
++		*/
++		skb->truesize = SKB_TRUESIZE(SKB_DATA_ALIGN(len + NET_SKB_PAD));
++		skb->fast_recycled = 0;
++		return skb;
++	}
++
++	len = SKB_RECYCLE_SIZE;
++	if (unlikely(length > SKB_RECYCLE_SIZE))
++		len = length;
++
++	skb = __alloc_skb(len + NET_SKB_PAD, gfp_mask,
++				SKB_ALLOC_RX, NUMA_NO_NODE);
++	if (!skb)
++		return NULL;
++
++	/* Set truesize as the needed buffer size
++	* rather than the allocated size by __alloc_skb().
++	* */
++	if (length + NET_SKB_PAD < SKB_WITH_OVERHEAD(PAGE_SIZE))
++		skb->truesize = SKB_TRUESIZE(SKB_DATA_ALIGN(length + NET_SKB_PAD));
++
++	skb_reserve(skb, NET_SKB_PAD);
++	skb->dev = dev;
++	return skb;
++}
++EXPORT_SYMBOL(__netdev_alloc_skb_no_skb_reset);
++#else
++struct sk_buff *__netdev_alloc_skb_no_skb_reset(struct net_device *dev,
++						unsigned int length, gfp_t gfp_mask)
++{
++	return __netdev_alloc_skb(dev, length, gfp_mask);
++}
++EXPORT_SYMBOL(__netdev_alloc_skb_no_skb_reset);
++#endif
++
+ /**
+  *	__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance
+  *	@napi: napi instance this buffer was allocated for
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950314-net-Fix-kernel-option-check-to-enable-skb-recycler-c.patch	2024-12-16 20:31:56.619134847 +0800
@@ -0,0 +1,55 @@
+From f3b4a6ff8f04cfdb8a845ce930db49b114a64a17 Mon Sep 17 00:00:00 2001
+From: Swati Singh <quic_swasing@quicinc.com>
+Date: Tue, 22 Aug 2023 17:39:12 +0530
+Subject: [PATCH 261/500] net: Fix kernel option check to enable skb recycler
+ code
+
+Use CONFIG_SKB_RECYCLER instead of CONFIG_TRACEPOINTS.
+
+Change-Id: Iad2d11ec90a305f8b170a15c7addb4d3e031adc3
+Signed-off-by: Swati Singh <quic_swasing@quicinc.com>
+---
+ include/linux/skbuff.h | 2 +-
+ net/core/skbuff.c      | 5 ++---
+ 2 files changed, 3 insertions(+), 4 deletions(-)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1248,7 +1248,7 @@ static inline void kfree_skb_list(struct
+ 	kfree_skb_list_reason(segs, SKB_DROP_REASON_NOT_SPECIFIED);
+ }
+ 
+-#ifdef CONFIG_TRACEPOINTS
++#ifdef CONFIG_SKB_RECYCLER
+ void consume_skb(struct sk_buff *skb);
+ #else
+ static inline void consume_skb(struct sk_buff *skb)
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -1411,7 +1411,6 @@ void skb_tx_error(struct sk_buff *skb)
+ }
+ EXPORT_SYMBOL(skb_tx_error);
+ 
+-#ifdef CONFIG_TRACEPOINTS
+ /**
+  *	consume_skb - free an skbuff
+  *	@skb: buffer to free
+@@ -1452,8 +1451,9 @@ void consume_skb(struct sk_buff *skb)
+ 	if (likely(skb_recycler_consume(skb)))
+ 		return;
+ 
++#ifdef CONFIG_TRACEPOINTS
+ 	trace_consume_skb(skb, __builtin_return_address(0));
+-
++#endif
+ 	/* We're not recycling so now we need to do the rest of what we would
+ 	 * have done in __kfree_skb (above and beyond the skb_release_head_state
+ 	 * that we already did).
+@@ -1462,7 +1462,6 @@ void consume_skb(struct sk_buff *skb)
+ 	kfree_skbmem(skb);
+ }
+ EXPORT_SYMBOL(consume_skb);
+-#endif
+ 
+ /**
+  *	consume_skb_list_fast - free a list of skbs
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950324-skbuff-Fix-the-skb-allocation-to-allocate-the-skbs-f.patch	2024-12-16 20:30:08.179888814 +0800
@@ -0,0 +1,85 @@
+From 73d8b70a8a32906ebf016fbb7a44115d8928c168 Mon Sep 17 00:00:00 2001
+From: Manish Verma <quic_maniverm@quicinc.com>
+Date: Tue, 10 Oct 2023 22:35:14 +0530
+Subject: [PATCH 263/500] [skbuff] Fix the skb allocation to allocate the skbs
+ from the SKB SLAB
+
+Due to the kmalloc_size_roundup() function added in the __alloc_skb()
+API in 6.1, this API is not allocating the SKBs from the NSS
+SKB SLAB area even when the request size is SKB_DATA_CACHE_SIZE.
+
+This change is deferring the kmalloc_size_roundup() function call after
+the SKB is allocated from the NSS SKB SLAB.
+
+Change-Id: Ic6d75d66163f677b12c915ee26afbbcb26536512
+Signed-off-by: Manish Verma <quic_maniverm@quicinc.com>
+---
+ net/core/skbuff.c | 28 ++++++++++++----------------
+ 1 file changed, 12 insertions(+), 16 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -582,19 +582,23 @@ static void *kmalloc_reserve(unsigned in
+ 	bool ret_pfmemalloc = false;
+ 	size_t obj_size;
+ 	void *obj;
++	struct kmem_cache * skb_cache;
+ 
+ 	obj_size = SKB_HEAD_ALIGN(*size);
+-	if (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE &&
+-	    !(flags & KMALLOC_NOT_NORMAL_BITS)) {
+-		obj = kmem_cache_alloc_node(skb_small_head_cache,
++	if ((obj_size <= SKB_SMALL_HEAD_CACHE_SIZE &&
++	    !(flags & KMALLOC_NOT_NORMAL_BITS)) ||
++	    (obj_size > SZ_2K && obj_size <= SKB_DATA_CACHE_SIZE)) {
++		skb_cache = (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE) ? skb_small_head_cache : skb_data_cache;
++		obj = kmem_cache_alloc_node(skb_cache,
+ 				flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+ 				node);
+-		*size = SKB_SMALL_HEAD_CACHE_SIZE;
++		*size = (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE) ? SKB_SMALL_HEAD_CACHE_SIZE : SKB_DATA_CACHE_SIZE;
++
+ 		if (obj || !(gfp_pfmemalloc_allowed(flags)))
+ 			goto out;
+ 		/* Try again but now we are using pfmemalloc reserves */
+ 		ret_pfmemalloc = true;
+-		obj = kmem_cache_alloc_node(skb_small_head_cache, flags, node);
++		obj = kmem_cache_alloc_node(skb_cache, flags, node);
+ 		goto out;
+ 	}
+ 
+@@ -608,12 +612,7 @@ static void *kmalloc_reserve(unsigned in
+ 	 * Try a regular allocation, when that fails and we're not entitled
+ 	 * to the reserves, fail.
+ 	 */
+-	if (size > SZ_2K && size <= SKB_DATA_CACHE_SIZE)
+-		obj = kmem_cache_alloc_node(skb_data_cache,
+-						flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+-						node);
+-	else
+-		obj = kmalloc_node_track_caller(obj_size,
++	obj = kmalloc_node_track_caller(obj_size,
+ 					flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+ 					node);
+ 	if (obj || !(gfp_pfmemalloc_allowed(flags)))
+@@ -621,10 +620,7 @@ static void *kmalloc_reserve(unsigned in
+ 
+ 	/* Try again but now we are using pfmemalloc reserves */
+ 	ret_pfmemalloc = true;
+-	if (size > SZ_2K && size <= SKB_DATA_CACHE_SIZE)
+-		obj = kmem_cache_alloc_node(skb_data_cache, flags, node);
+-	else
+-		obj = kmalloc_node_track_caller(obj_size, flags, node);
++	obj = kmalloc_node_track_caller(obj_size, flags, node);
+ 
+ out:
+ 	if (pfmemalloc)
+@@ -688,7 +684,7 @@ struct sk_buff *__alloc_skb(unsigned int
+ 	data = kmalloc_reserve(&size, gfp_mask, node, &pfmemalloc);
+ 	if (unlikely(!data))
+ 		goto nodata;
+-	/* kmalloc_size_roundup() might give us more room than requested.
++	/* kmalloc_reserve(size) might give us more room than requested.
+ 	 * Put skb_shared_info exactly at the end of allocated zone,
+ 	 * to allow max possible filling before reallocation.
+ 	 */
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950325-net-skbuff-Fix-compile-error-when-recycler-is-disabl.patch	2024-12-16 20:30:08.181888781 +0800
@@ -0,0 +1,33 @@
+From 4f4d584c0afc53c8337c5aeec1106e64559753bd Mon Sep 17 00:00:00 2001
+From: Swati Singh <quic_swasing@quicinc.com>
+Date: Tue, 5 Sep 2023 10:08:26 +0530
+Subject: [PATCH 262/500] net: skbuff: Fix compile error when recycler is
+ disabled
+
+consume_skb recycle function should be enclosed within
+CONFIG_SKB_RECYCLER macro.
+
+Change-Id: I1703919d8da10102951ec3795eb63cf7ecf9a44b
+Signed-off-by: Swati Singh <quic_swasing@quicinc.com>
+---
+ net/core/skbuff.c | 2 ++
+ 1 file changed, 2 insertions(+)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -1415,6 +1415,7 @@ EXPORT_SYMBOL(skb_tx_error);
+  *	Functions identically to kfree_skb, but kfree_skb assumes that the frame
+  *	is being dropped after a failure and notes that
+  */
++#ifdef CONFIG_SKB_RECYCLER
+ void consume_skb(struct sk_buff *skb)
+ {
+ 	if (!skb_unref(skb))
+@@ -1458,6 +1459,7 @@ void consume_skb(struct sk_buff *skb)
+ 	kfree_skbmem(skb);
+ }
+ EXPORT_SYMBOL(consume_skb);
++#endif
+ 
+ /**
+  *	consume_skb_list_fast - free a list of skbs
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950326-Add-notifier-interface.patch	2024-12-16 20:30:08.182888765 +0800
@@ -0,0 +1,103 @@
+From ee9d581bb6355f06df5d4c91f9de2aff894e7a88 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Thu, 27 Oct 2016 11:46:24 -0500
+Subject: [PATCH 258/500] Add notifier interface
+
+Add a notifier interface for the SKB recycler. This notifier will
+propagate events related to skb problems (double free, double alloc,
+checksum mismatch).
+
+The following change is merged together:
+
+	Don't call BUG_ON in the notifier
+
+	The notifier was meant to be terminal, i.e. after propagating the
+	event it stops the system.
+	This is causing confusion however, under the assumption that there is
+	an actual problem in the notifier code.
+
+	This commit removes the BUG_ON call in the notifier, so that the event
+	originator gets to stop the system (as it was done before the notifier
+	was introduced).
+
+	Change Id: I5a7af0374dc4991539d742712331de03aa3833d1
+	Author and Committer: Cristian Prundeanu <cprundea@codeaurora.org>
+
+Change-Id: I35262581ad964ef97d21946a965170531ca9034f
+Signed-off-by: Cristian Prundeanu <cprundea@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+Signed-off-by: Tian Yang <tiany@codeaurora.org>
+---
+ MAINTAINERS                  |  1 +
+ include/linux/debugobjects.h |  2 ++
+ lib/debugobjects.c           | 23 +++++++++++++++++++++++
+ net/core/Makefile            |  1 +
+ 4 files changed, 27 insertions(+)
+
+--- a/MAINTAINERS
++++ b/MAINTAINERS
+@@ -71,6 +71,7 @@ SKB RECYCLER SUPPORT
+ M:	Casey Chen <kexinc@codeaurora.org>
+ S:	Maintained
+ F:	net/core/skbuff_recycle.*
++F:	net/core/skbuff_notifier.*
+ 
+ 3C59X NETWORK DRIVER
+ M:	Steffen Klassert <klassert@kernel.org>
+--- a/include/linux/debugobjects.h
++++ b/include/linux/debugobjects.h
+@@ -68,6 +68,7 @@ extern void debug_object_init      (void
+ extern void
+ debug_object_init_on_stack(void *addr, const struct debug_obj_descr *descr);
+ extern int debug_object_activate  (void *addr, const struct debug_obj_descr *descr);
++extern int debug_object_get_state(void *addr);
+ extern void debug_object_deactivate(void *addr, const struct debug_obj_descr *descr);
+ extern void debug_object_destroy   (void *addr, const struct debug_obj_descr *descr);
+ extern void debug_object_free      (void *addr, const struct debug_obj_descr *descr);
+@@ -85,6 +86,7 @@ debug_object_active_state(void *addr, co
+ extern void debug_objects_early_init(void);
+ extern void debug_objects_mem_init(void);
+ #else
++static inline int debug_object_get_state(void *addr) { return 0; }
+ static inline void
+ debug_object_init      (void *addr, const struct debug_obj_descr *descr) { }
+ static inline void
+--- a/lib/debugobjects.c
++++ b/lib/debugobjects.c
+@@ -494,6 +494,29 @@ static struct debug_bucket *get_bucket(u
+ 	return &obj_hash[hash];
+ }
+ 
++/*
++ * debug_object_get_state():
++ *   returns the state of an object given an address
++ */
++int debug_object_get_state(void *addr)
++{
++	struct debug_bucket *db;
++	struct debug_obj *obj;
++	unsigned long flags;
++	enum debug_obj_state state = ODEBUG_STATE_NOTAVAILABLE;
++
++	db = get_bucket((unsigned long) addr);
++
++	raw_spin_lock_irqsave(&db->lock, flags);
++	obj = lookup_object(addr, db);
++	if (obj)
++		state = obj->state;
++	raw_spin_unlock_irqrestore(&db->lock, flags);
++
++	return state;
++}
++EXPORT_SYMBOL(debug_object_get_state);
++
+ static void debug_print_object(struct debug_obj *obj, char *msg)
+ {
+ 	const struct debug_obj_descr *descr = obj->descr;
+--- a/net/core/Makefile
++++ b/net/core/Makefile
+@@ -42,3 +42,4 @@ obj-$(CONFIG_BPF_SYSCALL) += sock_map.o
+ obj-$(CONFIG_BPF_SYSCALL) += bpf_sk_storage.o
+ obj-$(CONFIG_OF)	+= of_net.o
+ obj-$(CONFIG_SKB_RECYCLER) += skbuff_recycle.o
++obj-$(CONFIG_DEBUG_OBJECTS_SKBUFF) += skbuff_notifier.o
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950327-net-skbuff-use-debug-objects-to-track-skb-allocation.patch	2024-12-16 20:30:08.184888733 +0800
@@ -0,0 +1,128 @@
+From 8c42ad24a4efaa2df444625fb1d6fc5dea5459b5 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Mon, 21 Sep 2015 18:50:13 -0500
+Subject: [PATCH 297/500] net: skbuff: use debug objects to track skb
+ allocations
+
+* tracks skb allocations and frees and warns / errors if
+  re-use occurs
+* init/destroy for slab allocations
+* activate/deactivate for in use
+
+Change-Id: Ia2dd0c7549d765a282295daf27bee6f99e5c7a43
+Signed-off-by: Matthew McClintock <mmcclint@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+Signed-off-by: Tian Yang <tiany@codeaurora.org>
+---
+ MAINTAINERS       | 1 +
+ lib/Kconfig.debug | 6 ++++++
+ net/core/Makefile | 2 +-
+ net/core/dev.c    | 1 +
+ net/core/skbuff.c | 9 ++++++++-
+ 5 files changed, 17 insertions(+), 2 deletions(-)
+
+--- a/MAINTAINERS
++++ b/MAINTAINERS
+@@ -72,6 +72,7 @@ M:	Casey Chen <kexinc@codeaurora.org>
+ S:	Maintained
+ F:	net/core/skbuff_recycle.*
+ F:	net/core/skbuff_notifier.*
++F:	net/core/skbuff_debug.*
+ 
+ 3C59X NETWORK DRIVER
+ M:	Steffen Klassert <klassert@kernel.org>
+--- a/lib/Kconfig.debug
++++ b/lib/Kconfig.debug
+@@ -741,6 +741,12 @@ config DEBUG_OBJECTS_PERCPU_COUNTER
+ 	  percpu counter routines to track the life time of percpu counter
+ 	  objects and validate the percpu counter operations.
+ 
++config DEBUG_OBJECTS_SKBUFF
++	bool "Debug sk_buff allocations"
++	depends on DEBUG_OBJECTS
++	help
++	  Enable this to turn on debugging of sk_buff's (incl. recycler)
++
+ config DEBUG_OBJECTS_ENABLE_DEFAULT
+ 	int "debug_objects bootup default value (0-1)"
+ 	range 0 1
+--- a/net/core/Makefile
++++ b/net/core/Makefile
+@@ -42,4 +42,4 @@ obj-$(CONFIG_BPF_SYSCALL) += sock_map.o
+ obj-$(CONFIG_BPF_SYSCALL) += bpf_sk_storage.o
+ obj-$(CONFIG_OF)	+= of_net.o
+ obj-$(CONFIG_SKB_RECYCLER) += skbuff_recycle.o
+-obj-$(CONFIG_DEBUG_OBJECTS_SKBUFF) += skbuff_notifier.o
++obj-$(CONFIG_DEBUG_OBJECTS_SKBUFF) += skbuff_debug.o skbuff_notifier.o
+--- a/net/core/dev.c
++++ b/net/core/dev.c
+@@ -157,6 +157,7 @@
+ 
+ #include "dev.h"
+ #include "net-sysfs.h"
++#include "skbuff_debug.h"
+ 
+ static DEFINE_SPINLOCK(ptype_lock);
+ struct list_head ptype_base[PTYPE_HASH_SIZE] __read_mostly;
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -117,6 +117,7 @@ struct kmem_cache *skb_data_cache;
+ #endif
+ 
+ #include "skbuff_recycle.h"
++#include "skbuff_debug.h"
+ 
+ static struct kmem_cache *skbuff_fclone_cache __ro_after_init;
+ #ifdef CONFIG_SKB_EXTENSIONS
+@@ -392,8 +393,8 @@ static inline void __finalize_skb_around
+ 	shinfo = skb_shinfo(skb);
+ 	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+ 	atomic_set(&shinfo->dataref, 1);
+-
+ 	skb_set_kcov_handle(skb, kcov_common_handle());
++	skbuff_debugobj_init_and_activate(skb);
+ }
+ 
+ static inline void *__slab_build_skb(struct sk_buff *skb, void *data,
+@@ -708,6 +709,7 @@ struct sk_buff *__alloc_skb(unsigned int
+ 		refcount_set(&fclones->fclone_ref, 1);
+ 	}
+ 
++	skbuff_debugobj_init_and_activate(skb);
+ 	return skb;
+ 
+ nodata:
+@@ -1149,6 +1151,7 @@ void kfree_skbmem(struct sk_buff *skb)
+ 
+ 	switch (skb->fclone) {
+ 	case SKB_FCLONE_UNAVAILABLE:
++		skbuff_debugobj_deactivate(skb);
+ 		kmem_cache_free(skbuff_cache, skb);
+ 		return;
+ 
+@@ -1169,7 +1172,9 @@ void kfree_skbmem(struct sk_buff *skb)
+ 	}
+ 	if (!refcount_dec_and_test(&fclones->fclone_ref))
+ 		return;
++
+ fastpath:
++	skbuff_debugobj_deactivate(&fclones->skb1);
+ 	kmem_cache_free(skbuff_fclone_cache, fclones);
+ }
+ 
+@@ -2127,6 +2132,7 @@ struct sk_buff *skb_clone(struct sk_buff
+ 			return NULL;
+ 
+ 		n->fclone = SKB_FCLONE_UNAVAILABLE;
++		skbuff_debugobj_init_and_activate(n);
+ 	}
+ 
+ 	return __skb_clone(n, skb);
+@@ -5957,6 +5963,7 @@ void kfree_skb_partial(struct sk_buff *s
+ 	if (head_stolen) {
+ 		skb_release_head_state(skb);
+ 		kmem_cache_free(skbuff_cache, skb);
++		skbuff_debugobj_deactivate(skb);
+ 	} else {
+ 		__kfree_skb(skb);
+ 	}
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950328-net-Check-if-sk_buff-head-is-valid-in-consume_skb.patch	2024-12-16 20:30:08.185888717 +0800
@@ -0,0 +1,51 @@
+From 71787b10a3e515685e209ca0a287be8002ba86f3 Mon Sep 17 00:00:00 2001
+From: Vivek Nataraja <nataraja@codeaurora.org>
+Date: Mon, 29 Jun 2015 11:31:50 +0530
+Subject: [PATCH 362/500] net: Check if sk_buff head is valid in consume_skb()
+
+Commit (0ebd0ac net: add function to allocate sk_buff head
+without data area) changes skb_release_all() to check if the
+skb has a data area to allow the skb destructor to clear the
+data pointer in case only a head has been allocated.
+
+This was later fixed to check skb->head instead of skb->data
+in 'commit 5e71d9d77c07 ("net: fix sk_buff head without data area")'.
+as skb->head points to the beginning of the data area.
+
+Since, this was done only in kfree_skb path and not in consume_skb
+path, this leads to kernel panic in some cases.
+
+Also, since skb_shared_info is not initialized in such skbs,
+accessing that in skb_recycler_consume() leads to kernel panic.
+
+Fix this by checking if skb->head is valid in both the cases.
+
+Change-Id: Iad9fcbaa91f5f1d1b8b00b88ae279b52db385f0c
+Signed-off-by: Vivek Nataraja <nataraja@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+---
+ net/core/skbuff.c | 6 ++++--
+ 1 file changed, 4 insertions(+), 2 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -1450,7 +1450,7 @@ void consume_skb(struct sk_buff *skb)
+ 	 * for us to recycle this one later than to allocate a new one
+ 	 * from scratch.
+ 	 */
+-	if (likely(skb_recycler_consume(skb)))
++	if (likely(skb->head) && likely(skb_recycler_consume(skb)))
+ 		return;
+ 
+ #ifdef CONFIG_TRACEPOINTS
+@@ -1460,7 +1460,9 @@ void consume_skb(struct sk_buff *skb)
+ 	 * have done in __kfree_skb (above and beyond the skb_release_head_state
+ 	 * that we already did).
+ 	 */
+-	skb_release_data(skb, SKB_CONSUMED, false);
++	if (likely(skb->head))
++		skb_release_data(skb, SKB_CONSUMED, false);
++
+ 	kfree_skbmem(skb);
+ }
+ EXPORT_SYMBOL(consume_skb);
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950329-skbuff_debug-track-functions-that-free-SKBs.patch	2024-12-16 20:31:57.373122673 +0800
@@ -0,0 +1,51 @@
+From 793b09101432ccc419e7ce572381accab552c442 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Thu, 1 Oct 2015 16:31:33 -0500
+Subject: [PATCH 358/500] skbuff_debug: track functions that free SKBs
+
+This adds a member to struct skbuff if
+CONFIG_DEBUG_OBJECTS_SKBUFF is turned on that tracks
+the caller that last free'd the SKB.
+
+This should work for SLAB and SKB recycler free since this
+element of the SKB should not get overwritten when allocated
+from either pool of objects.
+
+Performance info for before/after:
+
+Before:
+
+ speed was 53.45 Mbit/s
+ cycles = 97974050028 (per pkt = 35368.7482818)
+ instructions = 26722549696 (per pkt = 9646.87213988)
+ dcache_misses = 512380452 (per pkt = 184.969950983)
+ icache_misses = 1268005369 (per pkt = 457.75144238)
+
+After:
+
+ speed was 48.91 Mbit/s
+ cycles = 98737753294 (per pkt = 38614.9009095)
+ instructions = 25770915740 (per pkt = 10078.6307613)
+ dcache_misses = 589106068 (per pkt = 230.39082501)
+ icache_misses = 1146428790 (per pkt = 448.351645128)
+
+Change-Id: Ie9e854015dc040fc30b97f89dd0663601b8f0344
+Signed-off-by: Matthew McClintock <mmcclint@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+---
+ include/linux/skbuff.h | 4 ++++
+ 1 file changed, 4 insertions(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1063,6 +1063,10 @@ struct sk_buff {
+ 	/* only useable after checking ->active_extensions != 0 */
+ 	struct skb_ext		*extensions;
+ #endif
++
++#ifdef CONFIG_DEBUG_OBJECTS_SKBUFF
++	void			*free_addr;
++#endif
+ };
+ 
+ /* if you move pkt_type around you also must adapt those constants */
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950330-skbuff_debug-move-activate-deactivate-sites.patch	2024-12-16 20:30:08.188888668 +0800
@@ -0,0 +1,76 @@
+From 3a22b800812ab6b79b14114c3d14f4cc5293f7d4 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Tue, 29 Dec 2015 13:02:21 -0600
+Subject: [PATCH 359/500] skbuff_debug: move activate/deactivate sites
+
+This way we can see if next/prev in SKB was changed
+while the SKB was not being used.
+
+Change-Id: I281267e230d3406181a07d095a76ae6bc58e2c8d
+Signed-off-by: Matthew McClintock <mmcclint@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+Signed-off-by: Tian Yang <tiany@codeaurora.org>
+---
+ net/core/skbuff.c | 10 ++++++----
+ 1 file changed, 6 insertions(+), 4 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -394,7 +394,6 @@ static inline void __finalize_skb_around
+ 	memset(shinfo, 0, offsetof(struct skb_shared_info, dataref));
+ 	atomic_set(&shinfo->dataref, 1);
+ 	skb_set_kcov_handle(skb, kcov_common_handle());
+-	skbuff_debugobj_init_and_activate(skb);
+ }
+ 
+ static inline void *__slab_build_skb(struct sk_buff *skb, void *data,
+@@ -479,6 +478,7 @@ struct sk_buff *__build_skb(void *data,
+ 	skb = kmem_cache_alloc(skbuff_cache, GFP_ATOMIC);
+ 	if (unlikely(!skb))
+ 		return NULL;
++	skbuff_debugobj_init_and_activate(skb);
+ 
+ 	memset(skb, 0, offsetof(struct sk_buff, tail));
+ 	__build_skb_around(skb, data, frag_size);
+@@ -675,6 +675,7 @@ struct sk_buff *__alloc_skb(unsigned int
+ 		skb = kmem_cache_alloc_node(cache, gfp_mask & ~GFP_DMA, node);
+ 	if (unlikely(!skb))
+ 		return NULL;
++	skbuff_debugobj_init_and_activate(skb);
+ 	prefetchw(skb);
+ 
+ 	/* We do our best to align skb_shared_info on a separate cache
+@@ -709,10 +710,10 @@ struct sk_buff *__alloc_skb(unsigned int
+ 		refcount_set(&fclones->fclone_ref, 1);
+ 	}
+ 
+-	skbuff_debugobj_init_and_activate(skb);
+ 	return skb;
+ 
+ nodata:
++	skbuff_debugobj_deactivate(skb);
+ 	kmem_cache_free(cache, skb);
+ 	return NULL;
+ }
+@@ -2132,9 +2133,9 @@ struct sk_buff *skb_clone(struct sk_buff
+ 		n = kmem_cache_alloc(skbuff_cache, gfp_mask);
+ 		if (!n)
+ 			return NULL;
++		skbuff_debugobj_init_and_activate(n);
+ 
+ 		n->fclone = SKB_FCLONE_UNAVAILABLE;
+-		skbuff_debugobj_init_and_activate(n);
+ 	}
+ 
+ 	return __skb_clone(n, skb);
+@@ -5964,8 +5965,9 @@ void kfree_skb_partial(struct sk_buff *s
+ {
+ 	if (head_stolen) {
+ 		skb_release_head_state(skb);
+-		kmem_cache_free(skbuff_cache, skb);
++
+ 		skbuff_debugobj_deactivate(skb);
++		kmem_cache_free(skbuff_cache, skb);
+ 	} else {
+ 		__kfree_skb(skb);
+ 	}
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950331-skbuff_debug-track-alloc-and-free-complete-stacks.patch	2024-12-16 20:31:57.605118929 +0800
@@ -0,0 +1,28 @@
+From bd28f8e5c7e2a62d9019ab3e7dad5774b7a60dc2 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Tue, 29 Dec 2015 14:04:41 -0600
+Subject: [PATCH 360/500] skbuff_debug: track alloc and free complete stacks
+
+This adds support to record a complete stack trace for all
+alloc's and free's of SKBs.
+
+Change-Id: I81fc76240d49d18035e99c234abcb8e4b9cb14a5
+Signed-off-by: Matthew McClintock <mmcclint@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+---
+ include/linux/skbuff.h | 4 +++-
+ 1 file changed, 3 insertions(+), 1 deletion(-)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1065,7 +1065,9 @@ struct sk_buff {
+ #endif
+ 
+ #ifdef CONFIG_DEBUG_OBJECTS_SKBUFF
+-	void			*free_addr;
++#define DEBUG_OBJECTS_SKBUFF_STACKSIZE	20
++	void			*free_addr[DEBUG_OBJECTS_SKBUFF_STACKSIZE];
++	void			*alloc_addr[DEBUG_OBJECTS_SKBUFF_STACKSIZE];
+ #endif
+ };
+ 
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950332-skbuff_debug-track-sum-of-skb-while-not-in-use.patch	2024-12-16 20:31:57.719117088 +0800
@@ -0,0 +1,29 @@
+From e7578152d1a548a13fac7f9cf9853abd413b2482 Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Tue, 29 Dec 2015 15:20:54 -0600
+Subject: [PATCH 361/500] skbuff_debug: track sum of skb while not in use
+
+Add a simple sum check across the range of the SKB struct
+to see if something else modified after it was freed to the
+SKB recycler.
+
+Right now, we just check for the SKB recycler as freeing to
+the slab tends to let other changes to the struct occur.
+
+Change-Id: I41b4163bf735cc047dd7150cb009098665c9b94e
+Signed-off-by: Matthew McClintock <mmcclint@codeaurora.org>
+Signed-off-by: Casey Chen <kexinc@codeaurora.org>
+---
+ include/linux/skbuff.h | 1 +
+ 1 file changed, 1 insertion(+)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1068,6 +1068,7 @@ struct sk_buff {
+ #define DEBUG_OBJECTS_SKBUFF_STACKSIZE	20
+ 	void			*free_addr[DEBUG_OBJECTS_SKBUFF_STACKSIZE];
+ 	void			*alloc_addr[DEBUG_OBJECTS_SKBUFF_STACKSIZE];
++	u32			sum;
+ #endif
+ };
+ 
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950333-net-core-Disable-page-frag-allocations-for-SKB-for-2.patch	2024-12-16 20:31:57.839115151 +0800
@@ -0,0 +1,71 @@
+From 78165511f2ec240587843c3298a77913bfa56d1a Mon Sep 17 00:00:00 2001
+From: Tian Yang <tiany@codeaurora.org>
+Date: Tue, 30 Apr 2019 18:43:02 +0530
+Subject: [PATCH 363/500] net core: Disable page frag allocations for SKB for
+ 256MB profile
+
+For low memory profiles such as 256MB, using __alloc_page_frag()
+for skb allocations can potentially cause pages to be held up
+for longer duration without getting freed by the kernel memory
+manageer. This can potentially cause out-of-memory situaions.
+This patch disabled page frag based SKB allocations for such
+low memory profiles.
+
+Change-Id: Ibc674ad8b46211b8394f0bf7db55366d7210bb01
+Signed-off-by: Adil irfan <airfan@codeaurora.org>
+Signed-off-by: Kiran Kumar C.S.K <kkumarcs@codeaurora.org>
+---
+ net/Kconfig       | 8 +++++++-
+ net/core/skbuff.c | 8 +++++++-
+ 2 files changed, 14 insertions(+), 2 deletions(-)
+
+--- a/net/Kconfig
++++ b/net/Kconfig
+@@ -378,7 +378,6 @@ config SKB_RECYCLER
+ 	  routing workloads. It can reduce skbuff freeing or
+ 	  reallocation overhead.
+ 
+-
+ config SKB_RECYCLER_MULTI_CPU
+ 	bool "Cross-CPU recycling for CPU-locked workloads"
+ 	depends on SKB_RECYCLER
+@@ -402,6 +401,13 @@ config SKB_RECYCLE_MAX_PREALLOC_SKBS
+ 	help
+ 	 Number of SKBs each of 4K size to be preallocated for recycling
+ 
++config ALLOC_SKB_PAGE_FRAG_DISABLE
++	bool "Disable page fragment based skbuff payload allocations"
++	depends on !SKB_RECYCLER
++	default n
++	help
++	 Disable page fragment based allocations for skbuff payloads.
++
+ menu "Network testing"
+ 
+ config NET_PKTGEN
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -763,16 +763,22 @@ struct sk_buff *__netdev_alloc_skb(struc
+ #else
+ 	struct page_frag_cache *nc;
+ 	bool pfmemalloc;
++	bool page_frag_alloc_enable = true;
+ 	void *data;
+ 
+ 	len += NET_SKB_PAD;
+ 
++
++#ifdef CONFIG_ALLOC_SKB_PAGE_FRAG_DISABLE
++	page_frag_alloc_enable = false;
++#endif
+ 	/* If requested length is either too small or too big,
+ 	 * we use kmalloc() for skb->head allocation.
+ 	 */
+ 	if (len <= SKB_WITH_OVERHEAD(1024) ||
+ 	    len > SKB_WITH_OVERHEAD(PAGE_SIZE) ||
+-	    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA))) {
++	    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA)) ||
++	    !page_frag_alloc_enable) {
+ 		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);
+ 		if (!skb)
+ 			goto skb_fail;
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950334-arm-Fix-compilation-issue-on-512MB-32-bit-debug-buil.patch	2024-12-16 20:30:08.194888571 +0800
@@ -0,0 +1,26 @@
+From 2ce6b5ae1c4f4617b7c4f650c9dd4d27d4662d3c Mon Sep 17 00:00:00 2001
+From: Pavithra R <quic_pavir@quicinc.com>
+Date: Tue, 12 Sep 2023 17:53:01 +0530
+Subject: [PATCH 364/500] arm: Fix compilation issue on 512MB 32 bit debug
+ build
+
+Compiler is not able to find the definition for the parameter 'current'
+with 32bit build as 'current.h' for 32bit is not included in other
+header files. So including the header file to compile 32bit.
+
+Change-Id: I1e52bb48899c070ebac8909c43cdda5ee137a839
+Signed-off-by: Pavithra R <quic_pavir@quicinc.com>
+---
+ arch/arm/include/asm/stacktrace.h | 1 +
+ 1 file changed, 1 insertion(+)
+
+--- a/arch/arm/include/asm/stacktrace.h
++++ b/arch/arm/include/asm/stacktrace.h
+@@ -3,6 +3,7 @@
+ #define __ASM_STACKTRACE_H
+ 
+ #include <asm/ptrace.h>
++#include <asm/current.h>
+ #include <linux/llist.h>
+ 
+ struct stackframe {
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950335-net-Fix-the-crash-in-skbuff.c-for-debug-build.patch	2024-12-16 20:30:08.195888555 +0800
@@ -0,0 +1,26 @@
+From b2471098db7851fc1ccfc3cbeed4e64a7ae37ecd Mon Sep 17 00:00:00 2001
+From: Pavithra R <quic_pavir@quicinc.com>
+Date: Fri, 3 Nov 2023 22:43:32 +0530
+Subject: [PATCH 365/500] net: Fix the crash in skbuff.c for debug build
+
+When skb is allocated in NAPI context we aren't
+initializing and activating the skb object. Add support
+for init and activating skb in napi_build_skb.
+
+Change-Id: Idcc3d6c260831be12ec85df474bcdc17f86d3292
+Signed-off-by: Pavithra R <quic_pavir@quicinc.com>
+---
+ net/core/skbuff.c | 2 ++
+ 1 file changed, 2 insertions(+)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -541,6 +541,8 @@ static struct sk_buff *__napi_build_skb(
+ 	if (unlikely(!skb))
+ 		return NULL;
+ 
++	skbuff_debugobj_init_and_activate(skb);
++
+ 	memset(skb, 0, offsetof(struct sk_buff, tail));
+ 	__build_skb_around(skb, data, frag_size);
+ 
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950454-net-core-Do-not-change-the-truesize-after-skb-alloca.patch	2024-12-16 20:31:58.192109453 +0800
@@ -0,0 +1,53 @@
+From 2559972e90c586712a79e2195545d153aaae87cc Mon Sep 17 00:00:00 2001
+From: Pavithra R <quic_pavir@quicinc.com>
+Date: Wed, 13 Dec 2023 10:57:25 +0530
+Subject: [PATCH 449/500] net: core: Do not change the truesize after skb
+ allocation
+
+Do not change truesize after skb allocation.
+
+Change-Id: I2d64ad4eae990c7ca8a48433568635d0722d0ca1
+Signed-off-by: Pavithra R <quic_pavir@quicinc.com>
+---
+ net/core/skbuff.c | 16 ----------------
+ 1 file changed, 16 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -744,11 +744,6 @@ struct sk_buff *__netdev_alloc_skb(struc
+ 	bool reset_skb = true;
+ 	skb = skb_recycler_alloc(dev, length, reset_skb);
+ 	if (likely(skb)) {
+-		/* SKBs in the recycler are from various unknown sources.
+-		* Their truesize is unknown. We should set truesize
+-		* as the needed buffer size before using it.
+-		*/
+-		skb->truesize = SKB_TRUESIZE(SKB_DATA_ALIGN(len + NET_SKB_PAD));
+ 		skb->recycled_for_ds = 0;
+ 		return skb;
+ 	}
+@@ -855,11 +850,6 @@ struct sk_buff *__netdev_alloc_skb_no_sk
+ 
+ 	skb = skb_recycler_alloc(dev, length, reset_skb);
+ 	if (likely(skb)) {
+-		/* SKBs in the recycler are from various unknown sources.
+-		* Their truesize is unknown. We should set truesize
+-		* as the needed buffer size before using it.
+-		*/
+-		skb->truesize = SKB_TRUESIZE(SKB_DATA_ALIGN(len + NET_SKB_PAD));
+ 		skb->fast_recycled = 0;
+ 		return skb;
+ 	}
+@@ -873,12 +863,6 @@ struct sk_buff *__netdev_alloc_skb_no_sk
+ 	if (!skb)
+ 		return NULL;
+ 
+-	/* Set truesize as the needed buffer size
+-	* rather than the allocated size by __alloc_skb().
+-	* */
+-	if (length + NET_SKB_PAD < SKB_WITH_OVERHEAD(PAGE_SIZE))
+-		skb->truesize = SKB_TRUESIZE(SKB_DATA_ALIGN(length + NET_SKB_PAD));
+-
+ 	skb_reserve(skb, NET_SKB_PAD);
+ 	skb->dev = dev;
+ 	return skb;
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950498-net-Add-a-new-netdev_alloc_skb_fast-API-for-Data-pat.patch	2024-12-16 20:31:58.309107565 +0800
@@ -0,0 +1,198 @@
+From e864627b30cbafae11127a1b494250763785f02d Mon Sep 17 00:00:00 2001
+From: Nandha Kishore Easwaran <quic_nandhaki@quicinc.com>
+Date: Wed, 31 Jan 2024 11:38:54 +0530
+Subject: [PATCH] net: Add a new netdev_alloc_skb_fast API for Data path
+
+Add a new netdev_alloc_skb_fast API for Data path. This new API will
+be used by EDMA driver and Wifi driver to allocate SKBs where all fast
+recycle flags will be preserved. All fast_recycle flags will be reset
+when using netdev_alloc_skb.
+
+Change-Id: I4f9c9b5f04aaa1bd2a3dacccd9362ba9bdada364
+Signed-off-by: Nandha Kishore Easwaran <quic_nandhaki@quicinc.com>
+CRs-Fixed: 3666195
+---
+ include/linux/skbuff.h |  20 ++++++++
+ net/core/skbuff.c      | 114 ++++++++++++++++++++++++++++++++++++++++-
+ 2 files changed, 133 insertions(+), 1 deletion(-)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -3257,6 +3257,9 @@ static inline void *netdev_alloc_frag_al
+ struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int length,
+ 				   gfp_t gfp_mask);
+ 
++struct sk_buff *__netdev_alloc_skb_fast(struct net_device *dev, unsigned int length,
++				   gfp_t gfp_mask);
++
+ struct sk_buff *__netdev_alloc_skb_no_skb_reset(struct net_device *dev, unsigned int length,
+ 				   gfp_t gfp_mask);
+ 
+@@ -3279,6 +3282,23 @@ static inline struct sk_buff *netdev_all
+ 	return __netdev_alloc_skb(dev, length, GFP_ATOMIC);
+ }
+ 
++/**
++ *	netdev_alloc_skb_fast - allocate an skbuff for rx on a specific device
++ *	@dev: network device to receive on
++ *	@length: length to allocate
++ *
++ *      This API is same as netdev_alloc_skb except for the fact that it retains
++ *      the recycler fast flags.
++ *
++ *	%NULL is returned if there is no free memory. Although this function
++ *	allocates memory it can be called from an interrupt.
++ */
++static inline struct sk_buff *netdev_alloc_skb_fast(struct net_device *dev,
++						    unsigned int length)
++{
++	return __netdev_alloc_skb_fast(dev, length, GFP_ATOMIC);
++}
++
+ /* legacy helper around __netdev_alloc_skb() */
+ static inline struct sk_buff *__dev_alloc_skb(unsigned int length,
+ 					      gfp_t gfp_mask)
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -744,7 +744,7 @@ struct sk_buff *__netdev_alloc_skb(struc
+ 	bool reset_skb = true;
+ 	skb = skb_recycler_alloc(dev, length, reset_skb);
+ 	if (likely(skb)) {
+-		skb->recycled_for_ds = 0;
++		skb_recycler_clear_flags(skb);
+ 		return skb;
+ 	}
+ 
+@@ -822,6 +822,109 @@ skb_fail:
+ }
+ EXPORT_SYMBOL(__netdev_alloc_skb);
+ 
++/**
++ *	__netdev_alloc_skb_fast - allocate an skbuff for rx on a specific device
++ *	@dev: network device to receive on
++ *	@length: length to allocate
++ *	@gfp_mask: get_free_pages mask, passed to alloc_skb
++ *
++ *	Allocate a new &sk_buff and assign it a usage count of one. The
++ *	buffer has NET_SKB_PAD headroom built in. Users should allocate
++ *	the headroom they think they need without accounting for the
++ *	built in space. The built in space is used for optimisations.
++ *
++ *	%NULL is returned if there is no free memory.
++ */
++struct sk_buff *__netdev_alloc_skb_fast(struct net_device *dev,
++				   unsigned int length, gfp_t gfp_mask)
++{
++	struct sk_buff *skb;
++	unsigned int len = length;
++
++#ifdef CONFIG_SKB_RECYCLER
++	bool reset_skb = true;
++	skb = skb_recycler_alloc(dev, length, reset_skb);
++	if (likely(skb)) {
++		skb->recycled_for_ds = 0;
++		return skb;
++	}
++
++	len = SKB_RECYCLE_SIZE;
++	if (unlikely(length > SKB_RECYCLE_SIZE))
++		len = length;
++
++	skb = __alloc_skb(len + NET_SKB_PAD, gfp_mask,
++			  SKB_ALLOC_RX, NUMA_NO_NODE);
++	if (!skb)
++		goto skb_fail;
++
++	goto skb_success;
++#else
++	struct page_frag_cache *nc;
++	bool pfmemalloc;
++	bool page_frag_alloc_enable = true;
++	void *data;
++
++	len += NET_SKB_PAD;
++
++#ifdef CONFIG_ALLOC_SKB_PAGE_FRAG_DISABLE
++	page_frag_alloc_enable = false;
++#endif
++	/* If requested length is either too small or too big,
++	 * we use kmalloc() for skb->head allocation.
++	 */
++	if (len <= SKB_WITH_OVERHEAD(1024) ||
++	    len > SKB_WITH_OVERHEAD(PAGE_SIZE) ||
++	    (gfp_mask & (__GFP_DIRECT_RECLAIM | GFP_DMA)) ||
++		!page_frag_alloc_enable) {
++		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);
++		if (!skb)
++			goto skb_fail;
++		goto skb_success;
++	}
++
++	len += SKB_DATA_ALIGN(sizeof(struct skb_shared_info));
++	len = SKB_DATA_ALIGN(len);
++
++	if (sk_memalloc_socks())
++		gfp_mask |= __GFP_MEMALLOC;
++
++	if (in_irq() || irqs_disabled()) {
++		nc = this_cpu_ptr(&netdev_alloc_cache);
++		data = page_frag_alloc(nc, len, gfp_mask);
++		pfmemalloc = nc->pfmemalloc;
++	} else {
++		local_bh_disable();
++		nc = this_cpu_ptr(&napi_alloc_cache.page);
++		data = page_frag_alloc(nc, len, gfp_mask);
++		pfmemalloc = nc->pfmemalloc;
++		local_bh_enable();
++	}
++
++	if (unlikely(!data))
++		return NULL;
++
++	skb = __napi_build_skb(data, len);
++	if (unlikely(!skb)) {
++		skb_free_frag(data);
++		return NULL;
++	}
++
++	/* use OR instead of assignment to avoid clearing of bits in mask */
++	if (pfmemalloc)
++		skb->pfmemalloc = 1;
++	skb->head_frag = 1;
++#endif
++
++skb_success:
++	skb_reserve(skb, NET_SKB_PAD);
++	skb->dev = dev;
++
++skb_fail:
++	return skb;
++}
++EXPORT_SYMBOL(__netdev_alloc_skb_fast);
++
+ #ifdef CONFIG_SKB_RECYCLER
+ /* __netdev_alloc_skb_no_skb_reset - allocate an skbuff for rx on a specific device
+  *	@dev: network device to receive on
+@@ -1598,6 +1701,11 @@ static void __copy_skb_header(struct sk_
+ 	new->queue_mapping = old->queue_mapping;
+ 
+ 	memcpy(&new->headers, &old->headers, sizeof(new->headers));
++
++	/* Clear the skb recycler flags here to make sure any skb whose size
++	 * has been altered is not put back into recycler pool.
++	 */
++	skb_recycler_clear_flags(new);
+ 	CHECK_SKB_FIELD(protocol);
+ 	CHECK_SKB_FIELD(csum);
+ 	CHECK_SKB_FIELD(hash);
+@@ -2371,6 +2479,10 @@ int pskb_expand_head(struct sk_buff *skb
+ 	if (!skb->sk || skb->destructor == sock_edemux)
+ 		skb->truesize += size - osize;
+ 
++	/* Clear the skb recycler flags here to make sure any skb whose size
++	 * has been expanded is not put back into recycler.
++	 */
++	skb_recycler_clear_flags(skb);
+ 	return 0;
+ 
+ nofrags:
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950564-arm64-disable-DMA_BOUNCE_UNALIGNED_KMALLOC-for-ARCH_.patch	2024-12-16 20:30:08.199888490 +0800
@@ -0,0 +1,27 @@
+From 644a6afaef4c4e727256d0d1df59d0fd602ddba0 Mon Sep 17 00:00:00 2001
+From: Kathiravan Thirumoorthy <quic_kathirav@quicinc.com>
+Date: Tue, 4 Jun 2024 16:13:20 +0530
+Subject: [PATCH] arm64: disable DMA_BOUNCE_UNALIGNED_KMALLOC for ARCH_QCOM
+
+Enabling DMA_BOUNCE_UNALIGNED_KMALLOC reserves additional memory for
+the DMA bounce buffers, which is not needed for our use cases.
+
+Disable it to save considerable amount of RAM.
+
+Change-Id: Ice2240f30508d2315a57445db2649c1edf9b1914
+Signed-off-by: Kathiravan Thirumoorthy <quic_kathirav@quicinc.com>
+---
+ arch/arm64/Kconfig | 2 +-
+ 1 file changed, 1 insertion(+), 1 deletion(-)
+
+--- a/arch/arm64/Kconfig
++++ b/arch/arm64/Kconfig
+@@ -122,7 +122,7 @@ config ARM64
+ 	select CRC32
+ 	select DCACHE_WORD_ACCESS
+ 	select DYNAMIC_FTRACE if FUNCTION_TRACER
+-	select DMA_BOUNCE_UNALIGNED_KMALLOC
++	
+ 	select DMA_DIRECT_REMAP
+ 	select EDAC_SUPPORT
+ 	select FRAME_POINTER
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950574-skbuff_recycle-Mapping-dev_kfree-api-with-skb-recycl.patch	2024-12-16 20:31:58.551103658 +0800
@@ -0,0 +1,50 @@
+From 1969c1a046d0c9be1602ba356dbdc9b3ff430b7b Mon Sep 17 00:00:00 2001
+From: Swati Singh <quic_swasing@quicinc.com>
+Date: Wed, 12 Jun 2024 12:32:34 +0530
+Subject: [PATCH] [skbuff_recycle] Mapping dev_kfree api with skb recycler API.
+
+Currently, the __kfree_skb gets called for freeing the skb memory which
+will not put the SKB into recycler list.
+Hence based on conditional check whether recycler is enabled, function calls dev_kfree_skb
+api to return the skb back to recycler pool.
+
+APIs: dev_consume_skb_any : gets called form wifi driver to deal with
+skb memory.
+
+1. dev_kfree_skb_any_reason:
+   a. No memory will be consumed for recycler, if either it belongs to
+      interrupt context or critical section.
+   b. If the CONFIG_SKB_RECYCLER is not set, then free the
+      memory using linux APIs.
+   c. Else,
+      i. __kfree_skb_reason: This API gets called to check the reason for freeing.
+      ii. Raises warning if skb is not dropped yet and is attempted to drop.
+      iii. If the reason is SKB_CONSUMED, then trace_consume_skb gets
+           updated with reason.
+      iv. If the reason is not SUCCESS but others, then denotes a packet
+          drop and is monitored by trace_kfree_skb.
+      v. In either case, memory will be consumed for recycler.
+
+Change-Id: I3d0b2327f88ab27f299b93e251c53caebd0497de
+Signed-off-by: Swati Singh <quic_swasing@quicinc.com>
+---
+ net/core/skbuff.c | 7 ++++++-
+ 1 file changed, 6 insertions(+), 1 deletion(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -1342,8 +1342,13 @@ bool __kfree_skb_reason(struct sk_buff *
+ void __fix_address
+ kfree_skb_reason(struct sk_buff *skb, enum skb_drop_reason reason)
+ {
+-	if (__kfree_skb_reason(skb, reason))
++	if (__kfree_skb_reason(skb, reason)) {
++#if defined(CONFIG_SKB_RECYCLER)
++		dev_kfree_skb(skb);
++#else
+ 		__kfree_skb(skb);
++#endif
++	}
+ }
+ EXPORT_SYMBOL(kfree_skb_reason);
+ 
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950583-net-Enable-skb_recycler.patch	2024-12-16 20:30:08.202888441 +0800
@@ -0,0 +1,35 @@
+From 9c470c0b158473e94cc725f4b40cfdf0284706b3 Mon Sep 17 00:00:00 2001
+From: Swati Singh <quic_swasing@quicinc.com>
+Date: Tue, 14 Feb 2023 17:56:39 +0530
+Subject: [PATCH] net : Enable skb_recycler.
+
+The changes makes skb_recycler_size as configurable.
+If skb_recycler enabled, the size is set as,
+
+	1. 1792 for Alder and Miami boards with
+	   QSDK memory profile=512
+	2. 2304 for all premium and
+	   enterprize profiles.
+
+Change-Id: Ic353cf494854b681556bb451ba860a0e4e615947
+Signed-off-by: Swati Singh <quic_swasing@quicinc.com>
+---
+ net/Kconfig | 7 +++++++
+ 1 file changed, 7 insertions(+)
+
+--- a/net/Kconfig
++++ b/net/Kconfig
+@@ -401,6 +401,13 @@ config SKB_RECYCLE_MAX_PREALLOC_SKBS
+ 	help
+ 	 Number of SKBs each of 4K size to be preallocated for recycling
+ 
++config SKB_RECYCLE_SIZE
++	int "SKB recycler size"
++	depends on SKB_RECYCLER
++	default 2304
++	help
++	SKB recycler default size
++
+ config ALLOC_SKB_PAGE_FRAG_DISABLE
+ 	bool "Disable page fragment based skbuff payload allocations"
+ 	depends on !SKB_RECYCLER
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950608-net-skbuff-Create-a-new-kmem-cache-of-size-2100.patch	2024-12-16 20:31:58.787099849 +0800
@@ -0,0 +1,100 @@
+From 45f5dd7a29efc0067e232b40085bc06b2742f9b2 Mon Sep 17 00:00:00 2001
+From: Nandha Kishore Easwaran <quic_nandhaki@quicinc.com>
+Date: Thu, 11 Jul 2024 15:59:21 +0530
+Subject: [PATCH] net: skbuff: Create a new kmem cache of size 2100
+
+Create a new kmem cache of size 2100. This is needed mainly
+for low memory profiles where Copy engine needs to allocate
+skb of size 2100. SKB recycler is enabled in 256M and 512M platforms
+and hence adding this new cache will prevent these buffers from getting
+into recycler.
+
+Change-Id: I3a2bc6e95617d17066a37f9bbb146bf9815dd4f9
+Signed-off-by: Nandha Kishore Easwaran <quic_nandhaki@quicinc.com>
+---
+ net/core/skbuff.c | 46 +++++++++++++++++++++++++++++++++-------------
+ 1 file changed, 33 insertions(+), 13 deletions(-)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -92,27 +92,32 @@
+ struct kmem_cache *skbuff_cache __ro_after_init;
+ 
+ struct kmem_cache *skb_data_cache;
++struct kmem_cache *skb_data_cache_2100;
+ 
+-/*
+- * For low memory profile, NSS_SKB_FIXED_SIZE_2K is enabled and
+- * CONFIG_SKB_RECYCLER is disabled. For premium and enterprise profile
+- * CONFIG_SKB_RECYCLER is enabled and NSS_SKB_FIXED_SIZE_2K is disabled.
+- * Irrespective of NSS_SKB_FIXED_SIZE_2K enabled/disabled, the
+- * CONFIG_SKB_RECYCLER and __LP64__ determines the value of SKB_DATA_CACHE_SIZE
+- */
+ #if defined(CONFIG_SKB_RECYCLER)
++#define SKB_DATA_CACHE_SIZE (SKB_DATA_ALIGN(SKB_RECYCLE_SIZE + NET_SKB_PAD) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+ /*
+- * 2688 for 64bit arch, 2624 for 32bit arch
++ * Both caches are kept same size in 1G profile so that all
++ * skbs could be recycled. For 256M and 512M profiles, new slab of size
++ * 2100 is created.
+  */
+-#define SKB_DATA_CACHE_SIZE (SKB_DATA_ALIGN(SKB_RECYCLE_SIZE + NET_SKB_PAD) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++#if defined(CONFIG_SKB_RECYCLER)
++#define SKB_DATA_CACHE_SIZE_2100 (SKB_DATA_ALIGN(2100 + NET_SKB_PAD) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+ #else
++#define SKB_DATA_CACHE_SIZE_2100 SKB_DATA_CACHE_SIZE
++#endif
++#else /* CONFIG_SKB_RECYCLER */
+ /*
+- * 2368 for 64bit arch, 2176 for 32bit arch
++ * DATA CACHE is 2368 for 64bit arch, 2176 for 32bit arch
++ * DATA_CACHE_2100 is 2496 for 64bit arch, 2432 for 32bit arch
++ * DATA CACHE size should always be lesser than that of DATA_CACHE_2100 size
+  */
+ #if defined(__LP64__)
+ #define SKB_DATA_CACHE_SIZE ((SKB_DATA_ALIGN(1984 + NET_SKB_PAD)) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++#define SKB_DATA_CACHE_SIZE_2100 (SKB_DATA_ALIGN(2100 + NET_SKB_PAD) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+ #else
+ #define SKB_DATA_CACHE_SIZE ((SKB_DATA_ALIGN(1856 + NET_SKB_PAD)) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
++#define SKB_DATA_CACHE_SIZE_2100 (SKB_DATA_ALIGN(2100 + NET_SKB_PAD) + SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+ #endif
+ #endif
+ 
+@@ -590,12 +595,22 @@ static void *kmalloc_reserve(unsigned in
+ 	obj_size = SKB_HEAD_ALIGN(*size);
+ 	if ((obj_size <= SKB_SMALL_HEAD_CACHE_SIZE &&
+ 	    !(flags & KMALLOC_NOT_NORMAL_BITS)) ||
+-	    (obj_size > SZ_2K && obj_size <= SKB_DATA_CACHE_SIZE)) {
+-		skb_cache = (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE) ? skb_small_head_cache : skb_data_cache;
++	    (obj_size > SZ_2K && obj_size <= SKB_DATA_CACHE_SIZE_2100)) {
++		if (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE)
++			skb_cache = skb_small_head_cache;
++		else if (obj_size <= SKB_DATA_CACHE_SIZE)
++			skb_cache = skb_data_cache;
++		else
++			skb_cache = skb_data_cache_2100;
+ 		obj = kmem_cache_alloc_node(skb_cache,
+ 				flags | __GFP_NOMEMALLOC | __GFP_NOWARN,
+ 				node);
+-		*size = (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE) ? SKB_SMALL_HEAD_CACHE_SIZE : SKB_DATA_CACHE_SIZE;
++		if (obj_size <= SKB_SMALL_HEAD_CACHE_SIZE)
++			*size = SKB_SMALL_HEAD_CACHE_SIZE;
++		else if (obj_size <= SKB_DATA_CACHE_SIZE)
++			*size = SKB_DATA_CACHE_SIZE;
++		else
++			*size = SKB_DATA_CACHE_SIZE_2100;
+ 
+ 		if (obj || !(gfp_pfmemalloc_allowed(flags)))
+ 			goto out;
+@@ -5186,6 +5201,11 @@ void __init skb_init(void)
+ 						0, SLAB_PANIC, 0, SKB_DATA_CACHE_SIZE,
+ 						NULL);
+ 
++	skb_data_cache_2100 = kmem_cache_create_usercopy("skb_data_cache_2100",
++						SKB_DATA_CACHE_SIZE_2100,
++						0, SLAB_PANIC, 0, SKB_DATA_CACHE_SIZE_2100,
++						NULL);
++
+ 	skbuff_cache = kmem_cache_create_usercopy("skbuff_head_cache",
+ 					      sizeof(struct sk_buff),
+ 					      0,
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950619-skb-recycler-update-the-kmemleak-status-in-recycler.patch	2024-12-16 20:31:58.921097685 +0800
@@ -0,0 +1,110 @@
+From 58f8f8ca551b02ae5be67b6261427db4476aad06 Mon Sep 17 00:00:00 2001
+From: Ken Zhu <quic_guigenz@quicinc.com>
+Date: Mon, 4 Dec 2023 11:15:26 -0800
+Subject: [PATCH] skb-recycler: update the kmemleak status in recycler
+
+free the kmemleak object of skb and its head when
+it is consumed by recycler.
+
+create the kmemleak object of skb and its head when
+it is allocated from recycler.
+
+Change-Id: Ibbf6a301202a04944b5e01a7dc9be5739b7c1e3b
+Signed-off-by: Ken Zhu <quic_guigenz@quicinc.com>
+---
+ include/linux/kmemleak.h |  4 ++++
+ mm/kmemleak.c            | 26 ++++++++++++++++++++++++++
+ net/core/skbuff.c        | 13 +++++++++++++
+ 3 files changed, 43 insertions(+)
+
+--- a/include/linux/kmemleak.h
++++ b/include/linux/kmemleak.h
+@@ -26,6 +26,7 @@ extern void kmemleak_free_part(const voi
+ extern void kmemleak_free_percpu(const void __percpu *ptr) __ref;
+ extern void kmemleak_update_trace(const void *ptr) __ref;
+ extern void kmemleak_not_leak(const void *ptr) __ref;
++extern void kmemleak_restore(const void *ptr, int min_count) __ref;
+ extern void kmemleak_ignore(const void *ptr) __ref;
+ extern void kmemleak_scan_area(const void *ptr, size_t size, gfp_t gfp) __ref;
+ extern void kmemleak_no_scan(const void *ptr) __ref;
+@@ -93,6 +94,9 @@ static inline void kmemleak_update_trace
+ static inline void kmemleak_not_leak(const void *ptr)
+ {
+ }
++static inline void kmemleak_restore(const void *ptr, int min_count)
++{
++}
+ static inline void kmemleak_ignore(const void *ptr)
+ {
+ }
+--- a/mm/kmemleak.c
++++ b/mm/kmemleak.c
+@@ -1139,6 +1139,32 @@ void __ref kmemleak_not_leak(const void
+ EXPORT_SYMBOL(kmemleak_not_leak);
+ 
+ /**
++ * kmemleak_restore - restore an allocated object ignored
++ * @ptr:	pointer to beginning of the object
++ * @min_count:	minimum number of references to this object.
++ *
++ * Calling this function on an object will cause the ignored memory block to be
++ * scanned and reported as a leak again.
++ */
++void __ref kmemleak_restore(const void *ptr, int min_count)
++{
++	pr_debug("%s(0x%p)\n", __func__, ptr);
++	if (kmemleak_enabled && ptr && !IS_ERR(ptr)) {
++		struct kmemleak_object *object;
++		object = find_and_get_object((unsigned long)ptr, 0);
++		if (!object) {
++			kmemleak_warn("Trying to restore unknown object at 0x%p\n",
++					ptr);
++			return;
++		}
++		paint_it(object, min_count);
++		object->flags &= ~OBJECT_NO_SCAN;
++		put_object(object);
++	}
++}
++EXPORT_SYMBOL(kmemleak_restore);
++
++/**
+  * kmemleak_ignore - ignore an allocated object
+  * @ptr:	pointer to beginning of the object
+  *
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -84,6 +84,7 @@
+ #include <linux/user_namespace.h>
+ #include <linux/indirect_call_wrapper.h>
+ #include <linux/textsearch.h>
++#include <linux/kmemleak.h>
+ 
+ #include "dev.h"
+ #include "sock_destructor.h"
+@@ -760,6 +761,12 @@ struct sk_buff *__netdev_alloc_skb(struc
+ 	skb = skb_recycler_alloc(dev, length, reset_skb);
+ 	if (likely(skb)) {
+ 		skb_recycler_clear_flags(skb);
++#ifdef CONFIG_DEBUG_KMEMLEAK
++		kmemleak_update_trace(skb);
++		kmemleak_restore(skb, 1);
++		kmemleak_update_trace(skb->head);
++		kmemleak_restore(skb->head, 1);
++#endif
+ 		return skb;
+ 	}
+ 
+@@ -968,6 +975,12 @@ struct sk_buff *__netdev_alloc_skb_no_sk
+ 
+ 	skb = skb_recycler_alloc(dev, length, reset_skb);
+ 	if (likely(skb)) {
++#ifdef CONFIG_DEBUG_KMEMLEAK
++		kmemleak_update_trace(skb);
++		kmemleak_restore(skb, 1);
++		kmemleak_update_trace(skb->head);
++		kmemleak_restore(skb->head, 1);
++#endif
+ 		skb->fast_recycled = 0;
+ 		return skb;
+ 	}
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950621-net-fix-callback-trace-issue-when-skb_debug_ojbect-i.patch	2024-12-16 20:31:59.040095765 +0800
@@ -0,0 +1,34 @@
+From e78b30af176e7a2f0307a45b0144749234cd10dc Mon Sep 17 00:00:00 2001
+From: Ken Zhu <quic_guigenz@quicinc.com>
+Date: Mon, 12 Aug 2024 17:35:35 -0700
+Subject: [PATCH] net: fix callback trace issue when skb_debug_ojbect is
+ enabled
+
+In the linux-6.6, there are some new APIs added, it free
+the skb to the skb cache directly without deactivate the
+debug object.
+
+Change-Id: I03516bfd0d389bbccb8de49b200bfe0a8703f4d1
+Signed-off-by: Ken Zhu <quic_guigenz@quicinc.com>
+---
+ net/core/skbuff.c | 2 ++
+ 1 file changed, 2 insertions(+)
+
+--- a/net/core/skbuff.c
++++ b/net/core/skbuff.c
+@@ -1419,6 +1419,7 @@ kfree_skb_list_reason(struct sk_buff *se
+ 
+ 		if (__kfree_skb_reason(segs, reason)) {
+ 			skb_poison_list(segs);
++			skbuff_debugobj_deactivate(segs);
+ 			kfree_skb_add_bulk(segs, &sa, reason);
+ 		}
+ 
+@@ -1655,6 +1656,7 @@ static void napi_skb_cache_put(struct sk
+ 	u32 i;
+ 
+ 	kasan_poison_object_data(skbuff_cache, skb);
++	skbuff_debugobj_deactivate(skb);
+ 	nc->skb_cache[nc->skb_count++] = skb;
+ 
+ 	if (unlikely(nc->skb_count == NAPI_SKB_CACHE_SIZE)) {
--- /dev/null	2024-12-14 07:49:50.161999989 +0800
+++ b/target/linux/generic/hack-6.6/950626-net-core-Add-default-disable-config-for-fast-recycle.patch	2024-12-16 20:31:59.163093779 +0800
@@ -0,0 +1,43 @@
+From c3765bc1333fbe89370f0a8b86a1ab8fe06678fa Mon Sep 17 00:00:00 2001
+From: Karthik T S <quic_kartikts@quicinc.com>
+Date: Thu, 8 Aug 2024 10:56:50 +0530
+Subject: [PATCH] net core: Add default disable config for fast recycled skbs
+ debug
+
+Add default disable config for fast recycled skbs debug
+
+Change-Id: I7cd85aa9609810b184d4d333483c7468fcee40fa
+Signed-off-by: Neha Bisht <quic_nbisht@quicinc.com>
+Signed-off-by: Karthik T S <quic_kartikts@quicinc.com>
+---
+ include/linux/skbuff.h | 2 +-
+ net/Kconfig            | 7 +++++++
+ 2 files changed, 8 insertions(+), 1 deletion(-)
+
+--- a/include/linux/skbuff.h
++++ b/include/linux/skbuff.h
+@@ -1395,7 +1395,7 @@ static inline int skb_pad(struct sk_buff
+ }
+ #define dev_kfree_skb(a)	consume_skb(a)
+ #define dev_kfree_skb_list_fast(a)	consume_skb_list_fast(a)
+-#if defined(SKB_FAST_RECYCLABLE_DEBUG_ENABLE) && defined(CONFIG_SKB_RECYCLER)
++#if defined(CONFIG_SKB_FAST_RECYCLABLE_DEBUG_ENABLE)
+ #define dev_check_skb_fast_recyclable(a)       check_skb_fast_recyclable(a)
+ #else
+ #define dev_check_skb_fast_recyclable(a)
+--- a/net/Kconfig
++++ b/net/Kconfig
+@@ -415,6 +415,13 @@ config ALLOC_SKB_PAGE_FRAG_DISABLE
+ 	help
+ 	 Disable page fragment based allocations for skbuff payloads.
+ 
++config SKB_FAST_RECYCLABLE_DEBUG_ENABLE
++	bool "Enable debug API for fast recycled skbs"
++	depends on SKB_RECYCLER
++	default n
++	help
++	 Enable debug API for fast recycled skbs.
++
+ menu "Network testing"
+ 
+ config NET_PKTGEN
